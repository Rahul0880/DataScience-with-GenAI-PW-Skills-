{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtPZEBIsVQ8D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "                        Theoritical Questions And Answers"
      ],
      "metadata": {
        "id": "8NihtLFTVkDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Can we use Bagging for regression problems?\n",
        "ans-Yes.\n",
        "Bagging (Bootstrap Aggregating) can be used for both classification and regression tasks.\n",
        "\n",
        "How it works in regression:\n",
        "\n",
        "Many regression models are trained on different bootstrap samples of the dataset.\n",
        "\n",
        "Each model outputs a numerical prediction.\n",
        "\n",
        "The final output is the average of all model predictions.\n",
        "\n",
        "Why bagging helps regression:\n",
        "\n",
        "It reduces variance, making the model less sensitive to noise.\n",
        "\n",
        "It improves stability and performance, especially for high-variance models like decision trees."
      ],
      "metadata": {
        "id": "5zKYJHP4VqEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.What is the difference between multiple model training and single model training?\n",
        "ans-Single Model Training\n",
        "\n",
        "You train one model on the entire dataset.\n",
        "\n",
        "Performance depends heavily on that model's:\n",
        "\n",
        "bias/variance characteristics\n",
        "\n",
        "sensitivity to noise\n",
        "\n",
        "ability to generalize\n",
        "\n",
        "If the model overfits or underfits, there's no backup.\n",
        "\n",
        "Multiple Model Training (Ensemble Learning)\n",
        "\n",
        "You train multiple models and combine their outputs.\n",
        "\n",
        "Includes methods like Bagging, Boosting, Random Forests, Stacking.\n",
        "\n",
        "Key Differences\n",
        "Aspect\tSingle Model\tMultiple Models\n",
        "Purpose\tLearn one best predictor\tCombine diverse models to improve performance\n",
        "Overfitting\tMore prone\tReduced (through averaging or boosting paradigms)\n",
        "Variance\tCan be high\tLower (especially in bagging)\n",
        "Bias\tFixed by model type\tCan be reduced with boosting\n",
        "Robustness\tLess robust\tMore robust to noise and outliers\n",
        "\n",
        "Summary: Multiple models often outperform a single model because they capture more patterns and reduce errors through combination.\n"
      ],
      "metadata": {
        "id": "30aTrmuLV4Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Explain the concept of feature randomness in Random Forests?\n",
        "ans-eature randomness is a core idea behind Random Forests.\n",
        "\n",
        "What it means:\n",
        "\n",
        "When splitting a node in each decision tree:\n",
        "\n",
        "Instead of considering all features, the algorithm considers only a random subset of features.\n",
        "\n",
        "The best split is chosen from that subset, not the entire set.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "It decorrelates the trees:\n",
        "Without feature randomness, many trees would look very similar (all using the strongest features first).\n",
        "\n",
        "It increases diversity among trees, which improves ensemble performance.\n",
        "\n",
        "It reduces overfitting while maintaining predictive power.\n",
        "\n",
        "Example:\n",
        "\n",
        "If you have 20 features and at each split you randomly choose 5 to evaluate:\n",
        "\n",
        "Tree 1 might evaluate features {1, 4, 7, 12, 13}\n",
        "\n",
        "Tree 2 might evaluate {2, 5, 9, 14, 18}\n",
        "‚Ä¶ and so on."
      ],
      "metadata": {
        "id": "takuisgPV-2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.What is OOB (Out-of-Bag) Score?\n",
        "ans-OOB score is a built-in validation method used in Bagging and Random Forests that acts like cross-validation without needing a separate validation set.\n",
        "\n",
        "How it works\n",
        "\n",
        "In Bagging/Random Forests, each tree is trained on a bootstrap sample (a random sample with replacement)\n",
        "\n",
        "On average, each bootstrap sample contains only ~63% of the training data.\n",
        "\n",
        "The remaining ~37% of data is not used to train that particular tree ‚Äî these samples are called Out-of-Bag samples.\n",
        "\n",
        "Computing the OOB score\n",
        "\n",
        "For each sample, predictions are made only by the trees that did NOT see that sample during training.\n",
        "\n",
        "These predictions are aggregated (majority vote or averaging).\n",
        "\n",
        "The aggregated prediction is compared to the true label.\n",
        "\n",
        "The overall accuracy/error obtained is the OOB score.\n",
        "\n",
        "Why OOB score is useful\n",
        "\n",
        "No need for a train-validation split ‚Üí more data remains for training.\n",
        "\n",
        "Gives an unbiased estimate of model performance.\n",
        "\n",
        "Very useful for Random Forests, where bootstrap sampling naturally occurs."
      ],
      "metadata": {
        "id": "5bDaTZY_WkEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.How can you measure the importance of features in a Random Forest model?\n",
        "ans-Random Forests provide two main ways to measure feature importance:\n",
        "\n",
        "A. Gini Importance (Mean Decrease in Impurity ‚Äî MDI)\n",
        "\n",
        "Default method used in many implementations (like scikit-learn).\n",
        "\n",
        "How it works\n",
        "\n",
        "Each time a feature is used to split a node in a tree, it reduces impurity (Gini, entropy, MSE for regression).\n",
        "\n",
        "The decrease in impurity is recorded.\n",
        "\n",
        "The total impurity decrease contributed by a feature is averaged over all trees.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Higher value = more important in reducing uncertainty.\n",
        "\n",
        "Pros\n",
        "\n",
        "Fast\n",
        "\n",
        "Built-in\n",
        "\n",
        "Cons\n",
        "\n",
        "Biased toward high-cardinality features (e.g., continuous features)\n",
        "\n",
        "B. Permutation Importance (Mean Decrease in Accuracy ‚Äî MDA)\n",
        "\n",
        "A more reliable, model-agnostic method.\n",
        "\n",
        "How it works\n",
        "\n",
        "Measure model accuracy on validation/OOB samples.\n",
        "\n",
        "Randomly shuffle one feature to break its relationship with the target.\n",
        "\n",
        "Re-measure accuracy.\n",
        "\n",
        "The drop in accuracy = importance of that feature.\n",
        "\n",
        "Interpretation\n",
        "\n",
        "If shuffling a feature causes a large drop in score ‚Üí it‚Äôs important.\n",
        "\n",
        "Pros\n",
        "\n",
        "More accurate, less biased\n",
        "\n",
        "Works for any model\n",
        "\n",
        "Cons\n",
        "\n",
        "Slower (requires multiple evaluations)\n",
        "\n",
        "Bonus: Feature Importance in Random Forest Summary\n",
        "Method\tName\tWhat It Measures\tPros\tCons\n",
        "MDI\tGini Importance\tReduction in impurity\tFast, built-in\tBiased for continuous/high-cardinality features\n",
        "MDA\tPermutation Importance\tDrop in accuracy when feature is shuffled\tMore reliable\tComputationally heavier"
      ],
      "metadata": {
        "id": "uKNWv1FGWn18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Explain the working principle of a Bagging Classifier?\n",
        "ans-1. Working Principle of a Bagging Classifier\n",
        "\n",
        "Bagging (Bootstrap Aggregating) is an ensemble technique designed to improve model stability and accuracy by reducing variance.\n",
        "\n",
        "How a Bagging Classifier Works\n",
        "\n",
        "Bootstrap Sampling\n",
        "\n",
        "From the original training dataset, create multiple new datasets by sampling with replacement.\n",
        "\n",
        "Each dataset is slightly different, typically containing around ~63% unique points from the original set.\n",
        "\n",
        "Train Multiple Base Models\n",
        "\n",
        "A base classifier (e.g., decision tree, SVM, logistic regression) is trained on each bootstrap sample.\n",
        "\n",
        "All models are trained independently and in parallel.\n",
        "\n",
        "Aggregation (Voting)\n",
        "\n",
        "For classification, predictions from all models are combined using:\n",
        "\n",
        "Majority voting (most common)\n",
        "\n",
        "Or weighted voting in some implementations\n",
        "\n",
        "Intuition\n",
        "\n",
        "Each model learns different patterns because it sees different data.\n",
        "\n",
        "Combining them cancels out individual model errors ‚Üí lower variance, better generalization."
      ],
      "metadata": {
        "id": "3qjw-CUAW-US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.How do you evaluate a Bagging Classifier‚Äôs performance?\n",
        "ans. How to Evaluate a Bagging Classifier‚Äôs Performance\n",
        "\n",
        "A Bagging Classifier is evaluated similarly to any classification model:\n",
        "\n",
        "A. Standard Evaluation Methods\n",
        "\n",
        "Train-test split accuracy\n",
        "\n",
        "Confusion matrix\n",
        "\n",
        "Precision, Recall, F1-score\n",
        "\n",
        "ROC-AUC\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "B. OOB (Out-of-Bag) Score (Specific to Bagging/Random Forest)\n",
        "\n",
        "No need for a validation set.\n",
        "\n",
        "Each sample is predicted only by models that did not train on it.\n",
        "\n",
        "Provides an unbiased estimate of test accuracy.\n",
        "\n",
        "C. Learning Curves\n",
        "\n",
        "Assess bias/variance trends and overfitting risk.\n",
        "\n",
        "D. Calibration plots (if probability estimates are needed)"
      ],
      "metadata": {
        "id": "ZFmXE8wgXJxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.How does a Bagging Regressor work?\n",
        "ans-A Bagging Regressor follows the same concept as the Bagging Classifier but uses regression models.\n",
        "\n",
        "Steps in a Bagging Regressor\n",
        "\n",
        "Bootstrap Sampling\n",
        "\n",
        "Multiple sampled datasets are created from the original data.\n",
        "\n",
        "Train Multiple Base Regressors\n",
        "\n",
        "Each bootstrap dataset trains a separate regression model (commonly DecisionTreeRegressor).\n",
        "\n",
        "Aggregation (Averaging)\n",
        "\n",
        "Each model outputs a numeric prediction.\n",
        "\n",
        "The final prediction is the mean (or sometimes median) of all model predictions.\n",
        "\n",
        "Why Bagging Helps in Regression?\n",
        "\n",
        "It reduces variance, which is crucial for high-variance models like decision trees.\n",
        "\n",
        "Provides smoother and more stable predictions."
      ],
      "metadata": {
        "id": "mKbTepwkXTE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9.What is the main advantage of ensemble techniques?\n",
        "ans-Main advantage: Better predictive performance through variance reduction and error cancellation.\n",
        "\n",
        "Ensemble techniques combine multiple models to:\n",
        "\n",
        "Reduce overfitting (variance reduction)\n",
        "\n",
        "Improve accuracy\n",
        "\n",
        "Increase model robustness\n",
        "\n",
        "Handle noisy data better\n",
        "\n",
        "Generalize well to unseen data\n",
        "\n",
        "Why this works\n",
        "\n",
        "Different models make different mistakes.\n",
        "When combined:\n",
        "\n",
        "Errors cancel out\n",
        "\n",
        "Strengths are amplified\n",
        "\n",
        "This leads to a model that is usually better than any individual member."
      ],
      "metadata": {
        "id": "W8dsS9yxXeNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. What is the main challenge of ensemble methods?\n",
        "ans-While ensemble methods are powerful, they come with some challenges:\n",
        "\n",
        "Main Challenges\n",
        "\n",
        "Increased Computational Cost\n",
        "\n",
        "Training many models requires more time and resources.\n",
        "\n",
        "Reduced Interpretability\n",
        "\n",
        "Ensembles act like ‚Äúblack boxes,‚Äù making it hard to understand why predictions were made.\n",
        "\n",
        "Risk of Overfitting (in some methods)\n",
        "\n",
        "Especially in boosting if the model is not regularized.\n",
        "\n",
        "Complexity in Deployment\n",
        "\n",
        "Multiple models make deployment and updates more complicated.\n",
        "\n",
        "Need for More Memory\n",
        "\n",
        "Storing many models increases memory usage."
      ],
      "metadata": {
        "id": "ZYtfK9mwX1gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11.Explain the key idea behind ensemble techniques?\n",
        "ans-Key Idea:\n",
        "\n",
        "Combine multiple weak or diverse models to create a stronger, more robust overall model.\n",
        "\n",
        "Why it works\n",
        "\n",
        "Different models capture different aspects of the data.\n",
        "\n",
        "Their individual errors tend to cancel out.\n",
        "\n",
        "The combined prediction is:\n",
        "\n",
        "More accurate\n",
        "\n",
        "Less sensitive to noise\n",
        "\n",
        "More stable\n",
        "\n",
        "Analogy\n",
        "\n",
        "Asking multiple experts and aggregating their opinions leads to a better decision than relying on only one expert.\n"
      ],
      "metadata": {
        "id": "OVdyeANlYAKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.What is a Random Forest Classifier?\n",
        "ans-A Random Forest Classifier is an ensemble learning method that uses:\n",
        "\n",
        "Bagging (Bootstrap Aggregation)\n",
        "\n",
        "Feature randomness\n",
        "\n",
        "to create a collection (‚Äúforest‚Äù) of decision trees.\n",
        "\n",
        "How it works\n",
        "\n",
        "Many decision trees are trained on different bootstrap samples of the data.\n",
        "\n",
        "At each split in each tree, only a random subset of features is considered.\n",
        "\n",
        "During prediction:\n",
        "\n",
        "Each tree votes for a class.\n",
        "\n",
        "The final output is decided by majority voting.\n",
        "\n",
        "Advantages\n",
        "\n",
        "High accuracy\n",
        "\n",
        "Robust to noise and overfitting\n",
        "\n",
        "Works well with high-dimensional data\n",
        "\n",
        "Provides feature importance scores"
      ],
      "metadata": {
        "id": "IEhq-_t1YF_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. What are the main types of ensemble techniques?\n",
        "ans-1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Trains multiple independent models in parallel.\n",
        "\n",
        "Uses bootstrap samples.\n",
        "\n",
        "Aggregates results by voting (classification) or averaging (regression).\n",
        "\n",
        "Examples:\n",
        "\n",
        "Bagging Classifier\n",
        "\n",
        "Random Forest\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "Models are trained sequentially.\n",
        "\n",
        "Each new model focuses on correcting the errors of the previous one.\n",
        "\n",
        "Reduces bias and increases accuracy.\n",
        "\n",
        "Examples:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM\n",
        "\n",
        "CatBoost\n",
        "\n",
        "3. Stacking (Stacked Generalization)\n",
        "\n",
        "Combines predictions of multiple base models using a meta-model.\n",
        "\n",
        "Learns how to optimally blend model outputs.\n",
        "\n",
        "Very flexible and powerful.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Stacked classifiers/regressors using logistic regression, linear regression, or other meta-learners.\n",
        "\n",
        "Bonus Category (sometimes separate):\n",
        "4. Voting Ensembles\n",
        "\n",
        "Combines multiple different models using simple majority vote or averaging.\n",
        "\n",
        "Unlike stacking, there is no learning in the combiner.\n"
      ],
      "metadata": {
        "id": "ImZyoCxjYMYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. What is ensemble learning in machine learning?\n",
        "ans-Ensemble learning is a machine learning technique where multiple models (often called base learners or weak learners) are combined to produce a stronger, more accurate, and more stable model.\n",
        "\n",
        "Why it works\n",
        "\n",
        "Different models make different errors.\n",
        "When combined, the ensemble:\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Reduces bias\n",
        "\n",
        "Improves generalization\n",
        "\n",
        "Becomes more robust to noise\n",
        "\n",
        "Common ensemble methods\n",
        "\n",
        "Bagging (e.g., Random Forest)\n",
        "\n",
        "Boosting (e.g., AdaBoost, XGBoost)\n",
        "\n",
        "Stacking\n",
        "\n",
        "Voting ensembles"
      ],
      "metadata": {
        "id": "iCEE9c1bYma3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15.When should we avoid using ensemble methods?\n",
        "ans-Although powerful, ensembles are not always appropriate. Avoid them when:\n",
        "\n",
        "A. The model must be highly interpretable\n",
        "\n",
        "Ensembles (like Random Forest, XGBoost) act like black boxes.\n",
        "\n",
        "B. Computational resources are limited\n",
        "\n",
        "They require:\n",
        "\n",
        "More training time\n",
        "\n",
        "More memory\n",
        "\n",
        "More processing power\n",
        "\n",
        "C. The dataset is very small\n",
        "\n",
        "Complex ensembles may overfit or give no improvement over simple models.\n",
        "\n",
        "D. Latency or deployment constraints\n",
        "\n",
        "If predictions must be extremely fast (edge devices, mobile), ensembles may be too heavy.\n",
        "\n",
        "E. A simple model already performs extremely well\n",
        "\n",
        "Unnecessarily adding complexity may not give meaningful gains."
      ],
      "metadata": {
        "id": "JnUgIrLvYv_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16.How does Bagging help in reducing overfitting?\n",
        "ans-Bagging reduces overfitting primarily by variance reduction.\n",
        "\n",
        "How Bagging reduces overfitting\n",
        "\n",
        "Creates multiple bootstrap samples\n",
        "Each sample is slightly different from the original dataset.\n",
        "\n",
        "Trains separate models independently\n",
        "Each model learns different patterns and noise.\n",
        "\n",
        "Aggregates predictions by averaging or voting\n",
        "This cancels out the individual model‚Äôs noise and overfitting tendencies.\n",
        "\n",
        "Why it works best with high-variance models\n",
        "\n",
        "Such as:\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Unpruned Tree-based models\n",
        "\n",
        "These models overfit easily, and Bagging stabilizes them."
      ],
      "metadata": {
        "id": "FPuK6wcSY4he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17.Why is Random Forest better than a single Decision Tree?\n",
        "ans-Random Forest improves over a single decision tree using two main mechanisms:\n",
        "\n",
        "A. Reduced Overfitting\n",
        "\n",
        "A single decision tree:\n",
        "\n",
        "Is very sensitive to noise\n",
        "\n",
        "Easily overfits\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Uses bagging ‚Üí reduces variance\n",
        "\n",
        "Uses feature randomness ‚Üí decorrelates trees\n",
        "\n",
        "Together, these produce more generalized predictions.\n",
        "\n",
        "B. Higher Accuracy\n",
        "\n",
        "Many weak learners combined produce a strong learner.\n",
        "\n",
        "C. More Robust to Noise and Outliers\n",
        "\n",
        "Errors from individual trees tend to cancel each other out.\n",
        "\n",
        "D. Better Feature Importance Measures\n",
        "\n",
        "Random Forest provides:\n",
        "\n",
        "Gini importance\n",
        "\n",
        "Permutation importance\n",
        "\n",
        "E. Less Sensitive to Hyperparameters\n",
        "\n",
        "A single tree needs careful tuning (depth, min samples).\n",
        "A Random Forest works well \"out of the box.\""
      ],
      "metadata": {
        "id": "-Rx3-tEjZEyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18. What is the role of bootstrap sampling in Bagging?\n",
        "ans-Bootstrap sampling is the core mechanism behind Bagging (Bootstrap Aggregating).\n",
        "\n",
        "Role of bootstrap sampling\n",
        "\n",
        "Creates multiple diverse datasets\n",
        "Each bootstrap sample is created by sampling with replacement, so every model sees a slightly different version of the original dataset.\n",
        "\n",
        "Introduces randomness ‚Üí reduces variance\n",
        "Since models train on different samples, they learn different patterns and make different errors. When combined, these errors cancel out.\n",
        "\n",
        "Allows independent parallel training\n",
        "Each model is trained independently on its own sample.\n",
        "\n",
        "Enables Out-of-Bag (OOB) evaluation\n",
        "About 37% of data is left out for each bootstrap, allowing an internal unbiased validation score without using a separate test set.\n",
        "\n",
        "In summary\n",
        "\n",
        "Bootstrap sampling makes Bagging powerful by:\n",
        "\n",
        "Increasing diversity\n",
        "\n",
        "Reducing overfitting\n",
        "\n",
        "Improving generalization"
      ],
      "metadata": {
        "id": "vIMs3RHPZgUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19.What are some real-world applications of ensemble techniques?\n",
        "ans-Ensemble methods are widely used because of their high accuracy and robustness.\n",
        "\n",
        "Real-World Applications\n",
        "A. Finance\n",
        "\n",
        "Credit scoring\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Risk assessment\n",
        "\n",
        "Stock price prediction\n",
        "\n",
        "B. Healthcare\n",
        "\n",
        "Disease diagnosis (e.g., cancer detection)\n",
        "\n",
        "Medical image analysis\n",
        "\n",
        "Predicting patient outcomes\n",
        "\n",
        "C. E-commerce\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Customer segmentation\n",
        "\n",
        "Churn prediction\n",
        "\n",
        "Personalized advertisements\n",
        "\n",
        "D. Cybersecurity\n",
        "\n",
        "Intrusion detection\n",
        "\n",
        "Anomaly detection in network traffic\n",
        "\n",
        "E. Natural Language Processing\n",
        "\n",
        "Spam filtering\n",
        "\n",
        "Sentiment analysis\n",
        "\n",
        "Document classification\n",
        "\n",
        "F. Manufacturing\n",
        "\n",
        "Predictive maintenance\n",
        "\n",
        "Quality control\n",
        "\n",
        "Fault detection\n",
        "\n",
        "G. Autonomous Vehicles\n",
        "\n",
        "Sensor fusion\n",
        "\n",
        "Pedestrian/object detection\n",
        "\n",
        "Path planning models\n",
        "\n",
        "In short:\n",
        "\n",
        "Anywhere accuracy and reliability matter, ensemble methods are used.\n",
        ""
      ],
      "metadata": {
        "id": "_QYaqZbFZpnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20.What is the difference between Bagging and Boosting?\n",
        "ans-Bagging\n",
        "Key Idea:\n",
        "\n",
        "Train many models independently and in parallel on different bootstrap samples, then combine results.\n",
        "\n",
        "Characteristics\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Base learners are trained in parallel\n",
        "\n",
        "All models have equal weight\n",
        "\n",
        "Uses bootstrap sampling\n",
        "\n",
        "Good for high-variance models (e.g., decision trees)\n",
        "\n",
        "Examples\n",
        "\n",
        "Bagging Classifier\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Boosting\n",
        "Key Idea:\n",
        "\n",
        "Train models sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
        "\n",
        "Characteristics\n",
        "\n",
        "Reduces bias AND variance\n",
        "\n",
        "Models are trained one after the other\n",
        "\n",
        "Later models get higher importance\n",
        "\n",
        "No bootstrap sampling (usually)\n",
        "\n",
        "Often achieves higher accuracy but risks overfitting\n",
        "\n",
        "Examples\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM\n",
        "\n",
        "CatBoost\n",
        "\n",
        "Side-by-Side Summary\n",
        "Aspect\tBagging\tBoosting\n",
        "Training\tParallel\tSequential\n",
        "Goal\tReduce variance\tReduce bias & variance\n",
        "Data Sampling\tBootstrap sampling\tNo/weighted sampling\n",
        "Model Focus\tIndependent models\tEach model fixes errors of previous\n",
        "Overfitting\tLess likely\tMore likely if not controlled\n",
        "Complexity\tLower\tHigher\n",
        "Accuracy\tGood\tOften excellent\n",
        ""
      ],
      "metadata": {
        "id": "Ufe--vAFZvqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                   PRACTICAL"
      ],
      "metadata": {
        "id": "lzpv_aPWaFbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy.\n",
        "\n",
        "ans-‚úÖ Train a Bagging Classifier Using Decision Trees (with Accuracy Score)"
      ],
      "metadata": {
        "id": "-Fe3R6CoaH5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Bagging Classifier with Decision Trees as base learners\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,              # number of trees\n",
        "    max_samples=0.8,              # fraction of dataset per tree\n",
        "    bootstrap=True,               # enable bootstrap sampling\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "5UJH5PDUanBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Explanation\n",
        "\n",
        "DecisionTreeClassifier is used as the base model.\n",
        "\n",
        "BaggingClassifier creates multiple bootstrap samples and trains many trees.\n",
        "\n",
        "Predictions are aggregated (majority vote).\n",
        "\n",
        "Accuracy is printed for the test set."
      ],
      "metadata": {
        "id": "gLPtMw6QajWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "ans-1. Train a Bagging Regressor Using Decision Trees + Evaluate with MSE"
      ],
      "metadata": {
        "id": "WkICdCppapJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load sample regression dataset\n",
        "# (Note: Boston dataset may be deprecated; replace with California Housing if needed)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Compute MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "WpoY4IYabWXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n",
        "ans-Train a Random Forest Classifier on the Breast Cancer Dataset + Print Feature Importances\n"
      ],
      "metadata": {
        "id": "UZhUlQJFa0Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Create a readable table\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Score': importances\n",
        "}).sort_values(by='Importance Score', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "S3Rg9cRnbL8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What These Scripts Do\n",
        "Bagging Regressor\n",
        "\n",
        "Uses DecisionTreeRegressor\n",
        "\n",
        "Trains 50 models on bootstrap samples\n",
        "\n",
        "Aggregates by averaging\n",
        "\n",
        "Evaluated with Mean Squared Error\n",
        "\n",
        "Random Forest Classifier\n",
        "\n",
        "Trained on Breast Cancer dataset\n",
        "\n",
        "Uses 100 decision trees\n",
        "\n",
        "Prints feature importance scores sorted from highest to lowest"
      ],
      "metadata": {
        "id": "cVwXBP5TbQE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24.Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "ans-Below is a complete Python example using scikit-learn that:\n",
        "\n",
        "‚úÖ Trains a Decision Tree Regressor\n",
        "‚úÖ Trains a Random Forest Regressor\n",
        "‚úÖ Evaluates both using Mean Squared Error (MSE)\n",
        "‚úÖ Compares their performance\n",
        "\n",
        "We will use the California Housing dataset (a regression dataset).\n",
        "\n",
        "‚úÖ Train Random Forest Regressor vs. Single Decision Tree Regressor"
      ],
      "metadata": {
        "id": "e73BJFgBbXuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Train a Single Decision Tree\n",
        "# -----------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Train a Random Forest\n",
        "# -----------------------------\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "# -----------------------------\n",
        "# Print comparison\n",
        "# -----------------------------\n",
        "print(f\"Decision Tree MSE:       {dt_mse:.4f}\")\n",
        "print(f\"Random Forest MSE:        {rf_mse:.4f}\")\n",
        "print(\"\\nPerformance Difference (DT - RF):\", round(dt_mse - rf_mse, 4))\n"
      ],
      "metadata": {
        "id": "ZuahtvmUbnX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Expected Interpretation\n",
        "\n",
        "Typically you will see something like:\n",
        "\n",
        "Decision Tree MSE: higher\n",
        "\n",
        "Random Forest MSE: much lower\n",
        "\n",
        "Why Random Forest performs better\n",
        "\n",
        "A single decision tree overfits easily (high variance).\n",
        "\n",
        "Random Forest averages many trees trained on different bootstrap samples, reducing variance.\n",
        "\n",
        "More stable and generalizes better."
      ],
      "metadata": {
        "id": "fsBKnXhsbtZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "ans-Below is a simple Python example using scikit-learn to compute the Out-of-Bag (OOB) score for a Random Forest Classifier.\n",
        "\n",
        "The OOB score is only available when:\n",
        "\n",
        "bootstrap=True (default)\n",
        "\n",
        "oob_score=True is explicitly enabled"
      ],
      "metadata": {
        "id": "EUpoRuT5bfpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split for good practice (although OOB doesn't require a test set)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest with OOB enabled\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    oob_score=True,       # Enable Out-of-Bag estimation\n",
        "    bootstrap=True,       # Must be True for OOB\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Print OOB score\n",
        "print(f\"OOB Score: {rf_clf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "3fXp-h3tb4bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå What the OOB Score Represents\n",
        "\n",
        "It is an internal validation score computed using only samples not used to train each tree.\n",
        "\n",
        "Works like built-in cross-validation without needing a separate validation set.\n",
        "\n",
        "Provides an unbiased estimate of the model's generalization performance."
      ],
      "metadata": {
        "id": "DfFkCERnb9Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#26. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n",
        "ans-Bagging Classifier with SVM Base Estimator"
      ],
      "metadata": {
        "id": "rw7aFdgbb-JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create Bagging Classifier with SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(\n",
        "    base_estimator=SVC(probability=True),  # SVM classifier\n",
        "    n_estimators=10,                       # number of base models\n",
        "    max_samples=0.8,                       # fraction of dataset per base model\n",
        "    bootstrap=True,                        # enable bootstrap sampling\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier (SVM) Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "WLyWdyuAcH3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Explanation\n",
        "\n",
        "SVC(probability=True) is used so Bagging can handle probability-based aggregation if needed.\n",
        "\n",
        "Bagging creates multiple bootstrap samples and trains an independent SVM on each.\n",
        "\n",
        "Predictions are combined via majority voting.\n",
        "\n",
        "Accuracy is calculated on the test set."
      ],
      "metadata": {
        "id": "QdmnqDTucLUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#27.Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "ans-Random Forest Classifier with Different Numbers of Trees"
      ],
      "metadata": {
        "id": "ent-HtOLcMRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Compare Random Forest with different numbers of trees\n",
        "n_trees_list = [10, 50, 100, 200]\n",
        "for n_trees in n_trees_list:\n",
        "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Random Forest with {n_trees} trees Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "2shfasdhct2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Compare how increasing trees improves stability and accuracy."
      ],
      "metadata": {
        "id": "eul00wPwcxvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28.Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n",
        "ans-Bagging Classifier with Logistic Regression (Compute AUC)\n",
        ""
      ],
      "metadata": {
        "id": "BRUVpcLecV0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Bagging with Logistic Regression\n",
        "bagging_lr = BaggingClassifier(\n",
        "    base_estimator=LogisticRegression(max_iter=1000),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_lr.fit(X_train, y_train)\n",
        "y_prob = bagging_lr.predict_proba(X_test)[:, 1]  # probability of positive class\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Bagging Classifier (Logistic Regression) AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6S4maPlc280"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Notes:\n",
        "\n",
        "predict_proba is required for AUC.\n",
        "\n",
        "Bagging reduces variance of logistic regression predictions slightly."
      ],
      "metadata": {
        "id": "EjvhAOEGc6V3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29.Train a Random Forest Regressor and analyze feature importance scores.\n",
        "ans-Random Forest Regressor + Feature Importance\n",
        ""
      ],
      "metadata": {
        "id": "YwaB_mVUccoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Load regression dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "importances = rf_reg.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nRandom Forest Regressor Feature Importances:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "YxD-UiY3c-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Identify which features contribute most to predictions."
      ],
      "metadata": {
        "id": "lPvL4U9kdCHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#30.Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "ans-Ensemble Model Combining Bagging and Random Forest (Compare Accuracy)\n",
        "\n",
        "One way to ‚Äúcombine‚Äù them is to create two models and compare their accuracy:"
      ],
      "metadata": {
        "id": "1KbPuAmzcjp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bagging Classifier with Decision Trees\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_pred = rf_clf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"\\nBagging (Decision Trees) Accuracy: {bagging_acc:.4f}\")\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "96UESzN_dFm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Notes:\n",
        "\n",
        "Bagging uses independent trees; Random Forest adds feature randomness, often slightly outperforming bagging alone.\n",
        "\n",
        "You can further combine predictions from both models via voting for a super-ensemble."
      ],
      "metadata": {
        "id": "UGnqg3K5dHjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#31.Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "ans-Random Forest Classifier + Hyperparameter Tuning with GridSearchCV"
      ],
      "metadata": {
        "id": "-w8s_5NldO20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest and hyperparameter grid\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train and find best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Test Set Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "okx5wjpfdeHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Notes:\n",
        "\n",
        "GridSearchCV searches combinations of hyperparameters and selects the one with the best cross-validation score.\n",
        "\n",
        "Key hyperparameters: n_estimators, max_depth, min_samples_split, min_samples_leaf."
      ],
      "metadata": {
        "id": "L-lO3qeZdgBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#32.Train a Bagging Regressor with different numbers of base estimators and compare performance.\n",
        "ans-Bagging Regressor with Different Numbers of Base Estimators"
      ],
      "metadata": {
        "id": "FZOls779dSnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Compare performance with different numbers of base estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bag_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=n_estimators,\n",
        "        max_samples=0.8,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Bagging Regressor with {n_estimators} trees MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "vCLvLQDddtMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Notes:\n",
        "\n",
        "Increasing the number of base estimators usually reduces variance, lowering MSE.\n",
        "\n",
        "Bagging is especially effective for high-variance models like decision trees."
      ],
      "metadata": {
        "id": "F1tT_aQGdncP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#33.Train a Random Forest Classifier and analyze misclassified samples\n",
        "ans-Random Forest Classifier + Analyze Misclassified Samples"
      ],
      "metadata": {
        "id": "zSSJht8YdvJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Random Forest Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Analyze misclassified samples\n",
        "misclassified_idx = X_test[y_test != y_pred]\n",
        "misclassified_true = y_test[y_test != y_pred]\n",
        "misclassified_pred = y_pred[y_test != y_pred]\n",
        "\n",
        "misclassified_df = pd.DataFrame(\n",
        "    misclassified_idx, columns=feature_names\n",
        ")\n",
        "misclassified_df['True Label'] = misclassified_true\n",
        "misclassified_df['Predicted Label'] = misclassified_pred\n",
        "\n",
        "print(\"\\nMisclassified Samples:\")\n",
        "print(misclassified_df)\n"
      ],
      "metadata": {
        "id": "-PjkRaNNeUey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Identify patterns in misclassified samples to understand model weaknesses."
      ],
      "metadata": {
        "id": "2l607SPQeX_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "ans-Bagging Classifier vs. Single Decision Tree\n"
      ],
      "metadata": {
        "id": "Ri7dTXU2d9zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Single Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    max_samples=0.8,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "V78zRg4hebaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes: Bagging usually improves accuracy by reducing variance of individual trees."
      ],
      "metadata": {
        "id": "FDK1pFzKec9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#35.Train a Random Forest Classifier and visualize the confusion matrix.\n",
        "ans-Random Forest Classifier + Confusion Matrix Visualization\n",
        ""
      ],
      "metadata": {
        "id": "1PdQk-NveECr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Random Forest Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "89BclwhVeh1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose: Visualize correct vs. misclassified samples per class."
      ],
      "metadata": {
        "id": "CpgY6yftejyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#36.Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n",
        "ans-Stacking Classifier: Decision Trees, SVM, Logistic Regression"
      ],
      "metadata": {
        "id": "A3wAsQxkeK8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Base learners\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Stacking Classifier with Logistic Regression as meta-model\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5\n",
        ")\n",
        "stack_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "stack_pred = stack_clf.predict(X_test)\n",
        "stack_acc = accuracy_score(y_test, stack_pred)\n",
        "\n",
        "print(f\"Stacking Classifier Accuracy: {stack_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ntbcUMTAeojm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Notes:\n",
        "\n",
        "Stacking combines predictions from multiple models to leverage their complementary strengths.\n",
        "\n",
        "Often achieves higher accuracy than any single base model."
      ],
      "metadata": {
        "id": "wJZr-ftweqNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#37. Train a Random Forest Classifier and print the top 5 most important features\n",
        "ans-Random Forest Classifier ‚Äì Top 5 Most Important Features"
      ],
      "metadata": {
        "id": "Rsbs8yqGesc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X, y)\n",
        "\n",
        "importances = rf_clf.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(importance_df.head(5))\n"
      ],
      "metadata": {
        "id": "tJs3ZVRvgBQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HOXN9wFSgC7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#38.Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n",
        "ans-Bagging Classifier ‚Äì Evaluate Precision, Recall, F1-score\n",
        ""
      ],
      "metadata": {
        "id": "fB3mIiqYe1di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "print(\"Classification Report (Bagging):\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "FNMDmALjgIXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#39.Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n",
        "ans-Random Forest Classifier ‚Äì Effect of max_depth on Accuracy\n",
        ""
      ],
      "metadata": {
        "id": "QDE7pn3ke65F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "max_depth_list = [None, 2, 4, 6, 8, 10]\n",
        "for depth in max_depth_list:\n",
        "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
        "    rf_clf.fit(X_train, y_train)\n",
        "    y_pred = rf_clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Max Depth: {depth}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "6Re6hIdhgPle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#40.Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare\n",
        "performance.\n",
        "ans-Bagging Regressor ‚Äì Compare Different Base Estimators\n",
        ""
      ],
      "metadata": {
        "id": "RS6o3NDVfBB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_h, y_h = housing.data, housing.target\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_h, y_h, test_size=0.2, random_state=42)\n",
        "\n",
        "base_estimators = {\n",
        "    'DecisionTree': DecisionTreeRegressor(),\n",
        "    'KNeighbors': KNeighborsRegressor()\n",
        "}\n",
        "\n",
        "for name, estimator in base_estimators.items():\n",
        "    bag_reg = BaggingRegressor(base_estimator=estimator, n_estimators=50, random_state=42)\n",
        "    bag_reg.fit(X_train_h, y_train_h)\n",
        "    y_pred = bag_reg.predict(X_test_h)\n",
        "    mse = mean_squared_error(y_test_h, y_pred)\n",
        "    print(f\"{name} Base Estimator Bagging MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "ylKcCthZgUhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#41.Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n",
        "ans-Random Forest Classifier ‚Äì ROC-AUC Score\n",
        ""
      ],
      "metadata": {
        "id": "zFRmfvbEfJpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_prob = rf_clf.predict_proba(X_test)[:,1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"Random Forest ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "6rOzxC6IgYow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#42.Train a Bagging Classifier and evaluate its performance using cross-validation.\n",
        "ans-Bagging Classifier ‚Äì Evaluate Using Cross-Validation\n",
        ""
      ],
      "metadata": {
        "id": "XYYzeMYJfXsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "cv_scores = cross_val_score(bag_clf, X, y, cv=5, scoring='accuracy')\n",
        "print(f\"Bagging Cross-Validation Accuracy: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n"
      ],
      "metadata": {
        "id": "_tYYJh64geJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#43.Train a Random Forest Classifier and plot the Precision-Recall curve.\n",
        "ans-Random Forest Classifier ‚Äì Precision-Recall Curve\n",
        ""
      ],
      "metadata": {
        "id": "mOtLaXDHfi0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_prob = rf_clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "disp.plot()\n",
        "plt.title(\"Random Forest Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ekzMpeHZgh9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#44.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n",
        "ans-Stacking Classifier ‚Äì Random Forest + Logistic Regression\n",
        ""
      ],
      "metadata": {
        "id": "TWSkbIS0fuER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "stack_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "stack_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stack_clf.predict(X_test)\n",
        "\n",
        "stack_acc = accuracy_score(y_test, y_pred_stack)\n",
        "print(f\"Stacking Classifier Accuracy: {stack_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "xDXaE5KbgmAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#45.Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "ans-Bagging Regressor ‚Äì Different Levels of Bootstrap Samples"
      ],
      "metadata": {
        "id": "AI0oDA0Rf2zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Different levels of bootstrap samples\n",
        "bootstrap_levels = [0.5, 0.7, 0.8, 1.0]\n",
        "\n",
        "# Compare performance\n",
        "for bs in bootstrap_levels:\n",
        "    bag_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=50,\n",
        "        max_samples=bs,   # fraction of samples per tree\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    bag_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions and MSE\n",
        "    y_pred = bag_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Bootstrap fraction: {bs}, Bagging Regressor MSE: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "WAF-UdewhDP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}