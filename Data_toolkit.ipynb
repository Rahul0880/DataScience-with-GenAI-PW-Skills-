{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science with Python: NumPy, Pandas, Matplotlib, Seaborn, and Plotly\n",
    "\n",
    "This notebook contains comprehensive answers to theoretical questions and practical exercises covering the essential Python libraries for data science.\n",
    "\n",
    "## Table of Contents\n",
    "1. [SKILLS - Theoretical Questions](#skills)\n",
    "2. [Practical - Coding Exercises](#practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e2a8",
   "metadata": {},
   "source": [
    "# SKILLS - Theoretical Questions\n",
    "\n",
    "## 1. What is NumPy, and why is it widely used in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61bbf3",
   "metadata": {},
   "source": [
    "NumPy (Numerical Python) is a fundamental library for scientific computing in Python. It provides:\n",
    "\n",
    "- **Efficient N-dimensional array objects**: NumPy arrays are homogeneous and stored in contiguous memory blocks, making operations faster than Python lists\n",
    "- **Mathematical functions**: Broadcasting, linear algebra, Fourier transforms, and random number generation\n",
    "- **Foundation for other libraries**: Pandas, Matplotlib, SciPy, and scikit-learn are built on NumPy\n",
    "- **Performance**: Written in C, NumPy operations are vectorized and significantly faster than pure Python\n",
    "- **Memory efficiency**: Uses less memory than Python lists due to homogeneous data types\n",
    "\n",
    "NumPy is widely used because it enables efficient numerical computations essential for data science, machine learning, and scientific computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c333f",
   "metadata": {},
   "source": [
    "## 2. How does broadcasting work in NumPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b5f0fe",
   "metadata": {},
   "source": [
    "Broadcasting is NumPy's ability to perform element-wise operations on arrays with different shapes without explicitly reshaping them. The rules are:\n",
    "\n",
    "1. **Shape comparison**: Arrays are aligned from the rightmost dimension\n",
    "2. **Dimension compatibility**: Dimensions are compatible if:\n",
    "   - They are equal, OR\n",
    "   - One of them is 1, OR\n",
    "   - One array has fewer dimensions (prepend 1s to its shape)\n",
    "3. **Result shape**: The resulting array has the maximum size along each dimension\n",
    "\n",
    "**Examples**:\n",
    "- `(3, 4) + (4,)` → broadcasts to `(3, 4) + (1, 4)` → result: `(3, 4)`\n",
    "- `(2, 3, 4) + (3, 1)` → broadcasts to `(2, 3, 4) + (1, 3, 1)` → result: `(2, 3, 4)`\n",
    "\n",
    "Broadcasting enables efficient operations without creating intermediate arrays, saving memory and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a4da9",
   "metadata": {},
   "source": [
    "## 3. What is a Pandas DataFrame?\n",
    "\n",
    "A Pandas DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. It's similar to:\n",
    "- A spreadsheet or SQL table\n",
    "- A dictionary of Series objects sharing the same index\n",
    "\n",
    "**Key characteristics**:\n",
    "- **Heterogeneous data**: Different columns can have different data types (int, float, string, etc.)\n",
    "- **Labeled axes**: Both rows (index) and columns have labels\n",
    "- **Size-mutable**: Rows and columns can be added or removed\n",
    "- **Data alignment**: Automatic alignment based on labels during operations\n",
    "\n",
    "DataFrames are the primary data structure in Pandas for data manipulation, analysis, and cleaning tasks.\n",
    "\n",
    "## 4. Explain the use of the groupby() method in Pandas.\n",
    "\n",
    "The `groupby()` method in Pandas splits data into groups based on specified criteria, applies functions to each group, and combines results. It implements the \"split-apply-combine\" strategy:\n",
    "\n",
    "1. **Split**: Divide data into groups based on one or more columns\n",
    "2. **Apply**: Apply a function (aggregation, transformation, or filtering) to each group\n",
    "3. **Combine**: Merge results into a new DataFrame/Series\n",
    "\n",
    "**Common uses**:\n",
    "- `df.groupby('column').sum()` - Sum values for each group\n",
    "- `df.groupby(['col1', 'col2']).mean()` - Multiple grouping columns\n",
    "- `df.groupby('column').agg({'col1': 'sum', 'col2': 'mean'})` - Different functions per column\n",
    "\n",
    "It's essential for data aggregation, statistical analysis, and creating summary reports.\n",
    "\n",
    "## 5. Why is Seaborn preferred for statistical visualizations?\n",
    "\n",
    "Seaborn is preferred because it:\n",
    "\n",
    "- **Built on Matplotlib**: Leverages Matplotlib's power while providing a higher-level interface\n",
    "- **Statistical focus**: Designed specifically for statistical data visualization\n",
    "- **Beautiful defaults**: Attractive color palettes and styling out-of-the-box\n",
    "- **Complex plots simplified**: Creates complex statistical plots with minimal code\n",
    "- **Data integration**: Works seamlessly with Pandas DataFrames\n",
    "- **Statistical functions**: Built-in statistical estimations and confidence intervals\n",
    "\n",
    "**Key advantages**:\n",
    "- Automatic handling of categorical data\n",
    "- Built-in themes and color palettes\n",
    "- Statistical plot types (violin plots, box plots, regression plots)\n",
    "- Easy faceting and subplot creation\n",
    "- Better handling of grouped data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65336dc0",
   "metadata": {},
   "source": [
    "## 6. What are the differences between NumPy arrays and Python lists?\n",
    "\n",
    "| Feature | NumPy Arrays | Python Lists |\n",
    "|---------|--------------|--------------|\n",
    "| **Data Types** | Homogeneous (single type) | Heterogeneous (mixed types) |\n",
    "| **Memory** | Contiguous memory block | Scattered memory locations |\n",
    "| **Performance** | Much faster for numerical operations | Slower for large datasets |\n",
    "| **Size** | Fixed size after creation | Dynamic size |\n",
    "| **Operations** | Vectorized operations | Element-by-element loops |\n",
    "| **Memory Usage** | More memory efficient | Higher memory overhead |\n",
    "| **Functionality** | Mathematical operations built-in | Limited mathematical functions |\n",
    "| **Broadcasting** | Supports broadcasting | No broadcasting |\n",
    "\n",
    "**Example**: Multiplying all elements by 2:\n",
    "- NumPy: `arr * 2` (vectorized, fast)\n",
    "- List: `[x * 2 for x in lst]` (requires loop, slower)\n",
    "\n",
    "## 7. What is a heatmap, and when should it be used?\n",
    "\n",
    "A heatmap is a data visualization technique that represents values in a matrix using colors. Darker/lighter colors or different hues represent higher/lower values.\n",
    "\n",
    "**When to use heatmaps**:\n",
    "- **Correlation matrices**: Show relationships between variables\n",
    "- **Confusion matrices**: Visualize classification model performance\n",
    "- **Time series data**: Display patterns over time and categories\n",
    "- **Geographic data**: Show data intensity across regions\n",
    "- **Large datasets**: Identify patterns in high-dimensional data\n",
    "- **Missing data patterns**: Visualize data completeness\n",
    "\n",
    "**Benefits**:\n",
    "- Quick pattern identification\n",
    "- Easy to spot outliers or clusters\n",
    "- Effective for large datasets\n",
    "- Intuitive color-based interpretation\n",
    "- Good for presentations and reports\n",
    "\n",
    "## 8. What does the term \"vectorized operation\" mean in NumPy?\n",
    "\n",
    "Vectorized operations in NumPy are operations that are applied element-wise to entire arrays without explicit Python loops. The operations are implemented in optimized C code.\n",
    "\n",
    "**Key characteristics**:\n",
    "- **No explicit loops**: Operations apply to entire arrays at once\n",
    "- **C-level implementation**: Much faster than Python loops\n",
    "- **Element-wise**: Operations performed on corresponding elements\n",
    "- **Broadcasting**: Automatic handling of different array shapes\n",
    "- **Memory efficient**: Minimal memory allocation during operations\n",
    "\n",
    "**Examples**:\n",
    "```python\n",
    "# Vectorized (fast)\n",
    "result = arr1 + arr2  # Adds corresponding elements\n",
    "result = np.sin(arr)  # Applies sin to all elements\n",
    "\n",
    "# Non-vectorized (slow)\n",
    "result = [arr1[i] + arr2[i] for i in range(len(arr1))]\n",
    "```\n",
    "\n",
    "This makes NumPy operations 10-100x faster than equivalent pure Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7550a5",
   "metadata": {},
   "source": [
    "## 9. How does Matplotlib differ from Plotly?\n",
    "\n",
    "| Aspect | Matplotlib | Plotly |\n",
    "|--------|------------|---------|\n",
    "| **Interactivity** | Static plots (by default) | Interactive plots (by default) |\n",
    "| **Learning Curve** | Steeper, more verbose | Gentler, more intuitive |\n",
    "| **Output** | Images (PNG, JPG, PDF, SVG) | HTML, web-based |\n",
    "| **Customization** | Highly customizable | Good customization with easier syntax |\n",
    "| **Performance** | Better for large datasets | Can be slower with huge datasets |\n",
    "| **3D Plotting** | Basic 3D capabilities | Excellent 3D visualization |\n",
    "| **Web Integration** | Requires additional work | Built for web deployment |\n",
    "| **Animation** | Possible but complex | Easy animation support |\n",
    "| **Ecosystem** | Mature, extensive ecosystem | Growing ecosystem |\n",
    "\n",
    "**Choose Matplotlib for**: Publication-quality static plots, fine-grained control, scientific publications\n",
    "**Choose Plotly for**: Interactive dashboards, web applications, business presentations\n",
    "\n",
    "## 10. What is the significance of hierarchical indexing in Pandas?\n",
    "\n",
    "Hierarchical indexing (MultiIndex) allows multiple index levels on axes, enabling:\n",
    "\n",
    "**Benefits**:\n",
    "- **Higher-dimensional data**: Represent 3D+ data in 2D DataFrames\n",
    "- **Grouping and aggregation**: Natural data organization for complex grouping\n",
    "- **Efficient storage**: Avoid creating separate DataFrames for different categories\n",
    "- **Easy subsetting**: Select data at different granularity levels\n",
    "- **Pivot operations**: Simplify reshaping between wide and long formats\n",
    "\n",
    "**Use cases**:\n",
    "- Time series with multiple frequencies (year, month, day)\n",
    "- Geographic data (country, state, city)\n",
    "- Experimental data (treatment, subject, measurement)\n",
    "- Financial data (asset, date, metric)\n",
    "\n",
    "**Example structure**:\n",
    "```\n",
    "Index levels:     Data\n",
    "Country  City     Population\n",
    "USA      NYC      8.4M\n",
    "         LA       3.9M\n",
    "UK       London   9.0M\n",
    "         Edinburgh 0.5M\n",
    "```\n",
    "\n",
    "## 11. What is the role of Seaborn's pairplot() function?\n",
    "\n",
    "The `pairplot()` function creates a grid of scatter plots for every pair of numerical variables in a dataset, with histograms on the diagonal.\n",
    "\n",
    "**Key features**:\n",
    "- **Pairwise relationships**: Shows correlation patterns between all variable pairs\n",
    "- **Distribution visualization**: Diagonal shows individual variable distributions\n",
    "- **Categorical grouping**: Can color-code points by categorical variables\n",
    "- **Quick exploration**: Rapid overview of multivariate relationships\n",
    "\n",
    "**When to use**:\n",
    "- Initial data exploration\n",
    "- Identifying correlations and patterns\n",
    "- Detecting outliers across multiple variables\n",
    "- Understanding data structure before modeling\n",
    "- Comparing groups within categorical variables\n",
    "\n",
    "**Benefits**:\n",
    "- Comprehensive view of dataset relationships\n",
    "- Easy identification of linear/non-linear patterns\n",
    "- Quick detection of clustering or separation by groups\n",
    "- Foundation for feature selection decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef3be4",
   "metadata": {},
   "source": [
    "## 12. What is the purpose of the describe() function in Pandas?\n",
    "\n",
    "The `describe()` function provides a comprehensive statistical summary of numerical data in a DataFrame or Series.\n",
    "\n",
    "**For numerical data, it returns**:\n",
    "- **count**: Number of non-null values\n",
    "- **mean**: Average value\n",
    "- **std**: Standard deviation\n",
    "- **min**: Minimum value\n",
    "- **25%**: First quartile (Q1)\n",
    "- **50%**: Median (Q2)\n",
    "- **75%**: Third quartile (Q3)\n",
    "- **max**: Maximum value\n",
    "\n",
    "**For categorical data** (when `include='object'`):\n",
    "- **count**: Number of non-null values\n",
    "- **unique**: Number of unique values\n",
    "- **top**: Most frequent value\n",
    "- **freq**: Frequency of the most common value\n",
    "\n",
    "**Uses**:\n",
    "- Quick data exploration and quality assessment\n",
    "- Identifying outliers and data distribution\n",
    "- Understanding data ranges and central tendencies\n",
    "- Checking for missing values and data completeness\n",
    "\n",
    "## 13. Why is handling missing data important in Pandas?\n",
    "\n",
    "Missing data handling is crucial because:\n",
    "\n",
    "**Impact on analysis**:\n",
    "- **Biased results**: Missing data can skew statistical measures\n",
    "- **Reduced sample size**: Decreases statistical power\n",
    "- **Algorithm failures**: Many ML algorithms can't handle NaN values\n",
    "- **Incorrect conclusions**: Missing patterns might be informative\n",
    "\n",
    "**Common strategies**:\n",
    "1. **Detection**: `isnull()`, `isna()`, `info()` to identify missing values\n",
    "2. **Removal**: `dropna()` to remove rows/columns with missing data\n",
    "3. **Imputation**: `fillna()` to replace with mean, median, mode, or forward/backward fill\n",
    "4. **Interpolation**: `interpolate()` for time series data\n",
    "5. **Indicator variables**: Create binary columns indicating missingness\n",
    "\n",
    "**Best practices**:\n",
    "- Understand why data is missing (MCAR, MAR, MNAR)\n",
    "- Choose appropriate strategy based on missing data mechanism\n",
    "- Document and justify missing data handling decisions\n",
    "- Consider multiple imputation for critical analyses\n",
    "\n",
    "## 14. What are the benefits of using Plotly for data visualization?\n",
    "\n",
    "**Key benefits**:\n",
    "\n",
    "1. **Interactivity by default**: Zoom, pan, hover, select without additional code\n",
    "2. **Web-ready**: Plots render in browsers, easy to share and embed\n",
    "3. **Professional appearance**: Publication-quality plots with minimal effort\n",
    "4. **3D visualization**: Excellent 3D plotting capabilities\n",
    "5. **Animation support**: Easy to create animated plots for time series\n",
    "6. **Cross-platform**: Works in Jupyter, web apps, desktop applications\n",
    "7. **Multiple languages**: Python, R, JavaScript, Julia support\n",
    "8. **Dashboard integration**: Easy integration with Dash for web apps\n",
    "\n",
    "**Specific advantages**:\n",
    "- **Hover information**: Rich tooltips with detailed data\n",
    "- **Responsive design**: Plots adapt to different screen sizes\n",
    "- **Export options**: Save as HTML, PNG, PDF, SVG\n",
    "- **Real-time updates**: Support for streaming data\n",
    "- **Statistical charts**: Built-in support for statistical visualizations\n",
    "- **Geographic mapping**: Excellent mapping capabilities\n",
    "\n",
    "**Use cases**: Interactive dashboards, web applications, business presentations, exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a951e",
   "metadata": {},
   "source": [
    "## 15. How does NumPy handle multidimensional arrays?\n",
    "\n",
    "NumPy handles multidimensional arrays through:\n",
    "\n",
    "**Array structure**:\n",
    "- **ndarray object**: N-dimensional array with homogeneous elements\n",
    "- **Shape**: Tuple describing array dimensions (e.g., (3, 4, 5) for 3D)\n",
    "- **Axes**: Each dimension is called an axis (axis 0, axis 1, etc.)\n",
    "- **Strides**: Memory layout information for efficient access\n",
    "\n",
    "**Key capabilities**:\n",
    "- **Flexible indexing**: `arr[i, j, k]` or `arr[i][j][k]`\n",
    "- **Slicing**: `arr[:, 1:3, :]` for multidimensional slices\n",
    "- **Broadcasting**: Operations across different shaped arrays\n",
    "- **Axis-specific operations**: `sum(axis=0)` operates along specific dimensions\n",
    "- **Reshaping**: `reshape()`, `flatten()`, `ravel()` for changing dimensions\n",
    "\n",
    "**Memory efficiency**:\n",
    "- Contiguous memory layout for fast access\n",
    "- Views vs copies for memory optimization\n",
    "- C and Fortran ordering options\n",
    "\n",
    "## 16. What is the role of Bokeh in data visualization?\n",
    "\n",
    "Bokeh is a Python library for creating interactive visualizations for web browsers.\n",
    "\n",
    "**Key features**:\n",
    "- **Web-native**: Generates JavaScript and HTML for web deployment\n",
    "- **Server applications**: Build interactive web applications\n",
    "- **Large datasets**: Handles big data efficiently with data decimation\n",
    "- **Streaming data**: Real-time data visualization capabilities\n",
    "- **Custom interactions**: Complex user interactions and widgets\n",
    "\n",
    "**Advantages**:\n",
    "- **Performance**: Efficient rendering of large datasets\n",
    "- **Flexibility**: From simple plots to complex applications\n",
    "- **Interactivity**: Rich interaction tools (selection, panning, zooming)\n",
    "- **Integration**: Works with NumPy, Pandas, and other Python libraries\n",
    "- **Deployment**: Easy web deployment without web development expertise\n",
    "\n",
    "**Use cases**:\n",
    "- Interactive dashboards for business intelligence\n",
    "- Real-time monitoring applications\n",
    "- Scientific data exploration tools\n",
    "- Financial trading interfaces\n",
    "- Geographic information systems\n",
    "\n",
    "## 17. Explain the difference between apply() and map() in Pandas.\n",
    "\n",
    "| Feature | apply() | map() |\n",
    "|---------|---------|--------|\n",
    "| **Scope** | DataFrames and Series | Series only |\n",
    "| **Functionality** | Can apply any function | Maps values using dict, Series, or function |\n",
    "| **Axis parameter** | Available for DataFrames (axis=0/1) | Not applicable |\n",
    "| **Performance** | Slower, more flexible | Faster for simple transformations |\n",
    "| **Return type** | Can return different types | Returns Series |\n",
    "| **Use case** | Complex transformations | Simple value mapping |\n",
    "\n",
    "**apply() examples**:\n",
    "```python\n",
    "df.apply(lambda x: x.sum(), axis=1)  # Row-wise sum\n",
    "df['col'].apply(lambda x: x**2)      # Square each value\n",
    "```\n",
    "\n",
    "**map() examples**:\n",
    "```python\n",
    "series.map({1: 'one', 2: 'two'})     # Dictionary mapping\n",
    "series.map(lambda x: x**2)           # Function mapping\n",
    "```\n",
    "\n",
    "**When to use**:\n",
    "- **map()**: Simple transformations, dictionary lookups, better performance\n",
    "- **apply()**: Complex functions, DataFrame operations, multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd30635",
   "metadata": {},
   "source": [
    "## 18. What are some advanced features of NumPy?\n",
    "\n",
    "**Advanced features include**:\n",
    "\n",
    "1. **Advanced indexing**:\n",
    "   - Boolean indexing: `arr[arr > 5]`\n",
    "   - Fancy indexing: `arr[[1, 3, 5]]`\n",
    "   - Multi-dimensional indexing: `arr[rows, cols]`\n",
    "\n",
    "2. **Broadcasting and vectorization**:\n",
    "   - Universal functions (ufuncs)\n",
    "   - Custom ufuncs with `numpy.vectorize()`\n",
    "   - Efficient element-wise operations\n",
    "\n",
    "3. **Linear algebra**:\n",
    "   - Matrix operations: `numpy.linalg`\n",
    "   - Eigenvalues, SVD, matrix decomposition\n",
    "   - Solving linear systems\n",
    "\n",
    "4. **Memory-mapped files**:\n",
    "   - `numpy.memmap` for large datasets\n",
    "   - Work with data larger than RAM\n",
    "\n",
    "5. **Structured arrays**:\n",
    "   - Arrays with named fields\n",
    "   - Mixed data types in single array\n",
    "\n",
    "6. **Performance optimization**:\n",
    "   - Views vs copies\n",
    "   - Memory layout optimization\n",
    "   - Integration with compiled code (Cython, Numba)\n",
    "\n",
    "## 19. How does Pandas simplify time series analysis?\n",
    "\n",
    "Pandas provides specialized tools for time series:\n",
    "\n",
    "**Time indexing**:\n",
    "- **DatetimeIndex**: Automatic date parsing and indexing\n",
    "- **Period and timedelta support**: Various time frequencies\n",
    "- **Time zone handling**: Localization and conversion\n",
    "\n",
    "**Resampling and grouping**:\n",
    "- `resample()`: Change frequency (daily to monthly)\n",
    "- `groupby()` with time periods\n",
    "- Automatic time-based aggregation\n",
    "\n",
    "**Missing data handling**:\n",
    "- Forward fill (`ffill`) and backward fill (`bfill`)\n",
    "- Interpolation methods for time series\n",
    "- Automatic handling of irregular time series\n",
    "\n",
    "**Time-based operations**:\n",
    "- Rolling windows: `rolling()` for moving averages\n",
    "- Expanding windows: `expanding()` for cumulative statistics\n",
    "- Time shifts: `shift()` for lag/lead operations\n",
    "\n",
    "**Benefits**:\n",
    "- Simplified date parsing and manipulation\n",
    "- Automatic alignment of time series data\n",
    "- Easy frequency conversion and resampling\n",
    "- Built-in time zone support\n",
    "- Integration with visualization libraries\n",
    "\n",
    "## 20. What is the role of a pivot table in Pandas?\n",
    "\n",
    "Pivot tables reorganize and summarize data by:\n",
    "\n",
    "**Functionality**:\n",
    "- **Reshaping**: Transform long-format data to wide-format\n",
    "- **Aggregation**: Group and summarize data automatically\n",
    "- **Cross-tabulation**: Create contingency tables\n",
    "- **Multi-level indexing**: Handle multiple grouping variables\n",
    "\n",
    "**Key parameters**:\n",
    "- `values`: Column to aggregate\n",
    "- `index`: Row grouping variable(s)\n",
    "- `columns`: Column grouping variable(s)\n",
    "- `aggfunc`: Aggregation function (mean, sum, count, etc.)\n",
    "\n",
    "**Use cases**:\n",
    "- Sales analysis by region and time period\n",
    "- Survey data analysis by demographics\n",
    "- Financial reporting by category and date\n",
    "- A/B testing results analysis\n",
    "\n",
    "**Benefits**:\n",
    "- Quick data summarization\n",
    "- Easy comparison across categories\n",
    "- Foundation for further analysis\n",
    "- Excel-like functionality in Python\n",
    "\n",
    "## 21. Why is NumPy's array slicing faster than Python's list slicing?\n",
    "\n",
    "**Reasons for speed advantage**:\n",
    "\n",
    "1. **Memory layout**:\n",
    "   - NumPy: Contiguous memory blocks\n",
    "   - Lists: Scattered memory locations with pointers\n",
    "\n",
    "2. **Implementation**:\n",
    "   - NumPy: C-level implementation\n",
    "   - Lists: Python-level with overhead\n",
    "\n",
    "3. **Views vs copies**:\n",
    "   - NumPy: Slicing creates views (shares memory)\n",
    "   - Lists: Slicing creates new copies\n",
    "\n",
    "4. **Data types**:\n",
    "   - NumPy: Homogeneous, fixed-size elements\n",
    "   - Lists: Heterogeneous, variable-size objects\n",
    "\n",
    "5. **Cache efficiency**:\n",
    "   - NumPy: Better CPU cache utilization\n",
    "   - Lists: Poor cache performance due to indirection\n",
    "\n",
    "**Performance difference**: NumPy slicing can be 10-100x faster for large arrays.\n",
    "\n",
    "## 22. What are some common use cases for Seaborn?\n",
    "\n",
    "**Statistical visualizations**:\n",
    "1. **Distribution plots**: Histograms, density plots, rug plots\n",
    "2. **Relationship plots**: Scatter plots with regression lines\n",
    "3. **Categorical plots**: Box plots, violin plots, bar plots\n",
    "4. **Matrix plots**: Heatmaps, correlation matrices\n",
    "\n",
    "**Specific use cases**:\n",
    "- **Exploratory data analysis**: Quick statistical summaries\n",
    "- **A/B testing**: Comparing distributions between groups\n",
    "- **Feature relationships**: Understanding variable correlations\n",
    "- **Data quality**: Identifying outliers and patterns\n",
    "- **Presentation**: Publication-ready statistical plots\n",
    "- **Regression analysis**: Visualizing model fits and residuals\n",
    "- **Time series**: Trend analysis and seasonal patterns\n",
    "- **Clustering**: Visualizing cluster separation and characteristics\n",
    "\n",
    "**Advantages for these use cases**:\n",
    "- Minimal code for complex statistical plots\n",
    "- Automatic statistical computations\n",
    "- Beautiful default styling\n",
    "- Integration with statistical testing\n",
    "- Easy handling of categorical data\n",
    "- Built-in support for grouped comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504441b4",
   "metadata": {},
   "source": [
    "# Practical - Coding Exercises\n",
    "\n",
    "## 1. How do you create a 2D NumPy array and calculate the sum of each row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 2D NumPy array\n",
    "array_2d = np.array([[1, 2, 3, 4],\n",
    "                     [5, 6, 7, 8],\n",
    "                     [9, 10, 11, 12]])\n",
    "\n",
    "print(\"Original 2D array:\")\n",
    "print(array_2d)\n",
    "\n",
    "# Calculate the sum of each row using axis=1\n",
    "row_sums = np.sum(array_2d, axis=1)\n",
    "\n",
    "print(\"\\nSum of each row:\")\n",
    "print(row_sums)\n",
    "\n",
    "# Alternative method using array.sum()\n",
    "row_sums_alt = array_2d.sum(axis=1)\n",
    "print(\"\\nUsing alternative method:\")\n",
    "print(row_sums_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3dc88",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `np.array()` creates a 2D NumPy array from a nested list\n",
    "- `axis=1` specifies that we want to sum along rows (axis 0 would sum along columns)\n",
    "- The result is a 1D array containing the sum of each row\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Original 2D array:\n",
    "[[ 1  2  3  4]\n",
    " [ 5  6  7  8]\n",
    " [ 9 10 11 12]]\n",
    "\n",
    "Sum of each row:\n",
    "[10 26 42]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6a0f2",
   "metadata": {},
   "source": [
    "## 2. Write a Pandas script to find the mean of a specific column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000],\n",
    "    'Score': [85.5, 92.0, 78.5, 88.0, 95.5]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Find the mean of specific columns\n",
    "age_mean = df['Age'].mean()\n",
    "salary_mean = df['Salary'].mean()\n",
    "score_mean = df['Score'].mean()\n",
    "\n",
    "print(f\"\\nMean of Age column: {age_mean}\")\n",
    "print(f\"Mean of Salary column: {salary_mean}\")\n",
    "print(f\"Mean of Score column: {score_mean}\")\n",
    "\n",
    "# Alternative way using describe() to get multiple statistics\n",
    "print(\"\\nUsing describe() for comprehensive statistics:\")\n",
    "print(df['Salary'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbfb5b6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `pd.DataFrame()` creates a DataFrame from a dictionary\n",
    "- `df['column_name'].mean()` calculates the mean of a specific column\n",
    "- The `.mean()` method automatically ignores NaN values\n",
    "- `describe()` provides comprehensive statistics including mean, std, min, max, and quartiles\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Mean of Age column: 30.0\n",
    "Mean of Salary column: 60000.0\n",
    "Mean of Score column: 87.9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3937ad",
   "metadata": {},
   "source": [
    "## 3. Create a scatter plot using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2135e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "x = np.random.normal(50, 15, 100)  # 100 points with mean=50, std=15\n",
    "y = 2 * x + np.random.normal(0, 10, 100)  # Linear relationship with noise\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, alpha=0.6, color='blue', s=50)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('X values', fontsize=12)\n",
    "plt.ylabel('Y values', fontsize=12)\n",
    "plt.title('Sample Scatter Plot', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add a trend line\n",
    "z = np.polyfit(x, y, 1)  # Linear fit\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, p(x), \"r--\", alpha=0.8, linewidth=2, label='Trend line')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation coefficient\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(f\"Correlation coefficient: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6d9d6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `plt.scatter()` creates the scatter plot with x and y coordinates\n",
    "- `alpha=0.6` makes points semi-transparent to show overlapping points\n",
    "- `s=50` sets the size of the scatter points\n",
    "- `np.polyfit()` and `np.poly1d()` create a linear trend line\n",
    "- `plt.grid()` adds a grid for better readability\n",
    "- `plt.tight_layout()` optimizes the spacing of plot elements\n",
    "\n",
    "**Expected Output:** A scatter plot showing the relationship between x and y variables with a red dashed trend line, demonstrating a positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f1a82",
   "metadata": {},
   "source": [
    "## 4. How do you calculate the correlation matrix using Seaborn and visualize it with a heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5679af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Height': np.random.normal(170, 10, 100),\n",
    "    'Weight': np.random.normal(70, 15, 100),\n",
    "    'Age': np.random.randint(18, 65, 100),\n",
    "    'Income': np.random.normal(50000, 20000, 100),\n",
    "    'Score': np.random.normal(75, 12, 100)\n",
    "}\n",
    "\n",
    "# Add some correlations to make it more interesting\n",
    "data['Weight'] = data['Height'] * 0.8 + np.random.normal(0, 5, 100)\n",
    "data['Income'] = data['Age'] * 800 + np.random.normal(0, 5000, 100)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Create heatmap using Seaborn\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,           # Show correlation values\n",
    "            cmap='coolwarm',      # Color palette\n",
    "            center=0,             # Center colormap at 0\n",
    "            square=True,          # Square cells\n",
    "            fmt='.3f',            # Format to 3 decimal places\n",
    "            linewidths=0.5,       # Add lines between cells\n",
    "            cbar_kws={'label': 'Correlation Coefficient'})\n",
    "\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742fdd1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- `df.corr()` calculates the Pearson correlation coefficient between all numeric columns\n",
    "- `sns.heatmap()` creates a color-coded matrix visualization\n",
    "- `annot=True` displays the correlation values on each cell\n",
    "- `cmap='coolwarm'` uses a blue-to-red color scheme (blue=negative, red=positive)\n",
    "- `center=0` centers the colormap at zero correlation\n",
    "- `fmt='.3f'` formats numbers to 3 decimal places\n",
    "\n",
    "**Expected Output:** A heatmap showing correlations between variables, with values ranging from -1 to 1. Strong positive correlations appear in red, strong negative in blue, and weak correlations in white."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495addc",
   "metadata": {},
   "source": [
    "## 5. Generate a bar plot using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data for bar plot\n",
    "categories = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E']\n",
    "sales_2022 = [120, 135, 98, 156, 142]\n",
    "sales_2023 = [145, 142, 110, 178, 155]\n",
    "\n",
    "# Method 1: Using Plotly Express (simpler)\n",
    "df_sales = pd.DataFrame({\n",
    "    'Product': categories * 2,\n",
    "    'Sales': sales_2022 + sales_2023,\n",
    "    'Year': ['2022'] * 5 + ['2023'] * 5\n",
    "})\n",
    "\n",
    "fig1 = px.bar(df_sales, \n",
    "              x='Product', \n",
    "              y='Sales', \n",
    "              color='Year',\n",
    "              title='Sales Comparison: 2022 vs 2023',\n",
    "              color_discrete_sequence=['lightblue', 'darkblue'])\n",
    "\n",
    "fig1.update_layout(\n",
    "    xaxis_title='Products',\n",
    "    yaxis_title='Sales (in thousands)',\n",
    "    font=dict(size=12),\n",
    "    title_font_size=16\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "# Method 2: Using Plotly Graph Objects (more control)\n",
    "fig2 = go.Figure()\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    name='2022',\n",
    "    x=categories,\n",
    "    y=sales_2022,\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    name='2023',\n",
    "    x=categories,\n",
    "    y=sales_2023,\n",
    "    marker_color='darkred'\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='Product Sales Comparison - Custom Styling',\n",
    "    xaxis_title='Products',\n",
    "    yaxis_title='Sales (in thousands)',\n",
    "    barmode='group',  # Side by side bars\n",
    "    template='plotly_white',\n",
    "    font=dict(size=12),\n",
    "    title_font_size=16\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(\"Data used for visualization:\")\n",
    "print(df_sales.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d46e0",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Method 1 (Plotly Express)**: Higher-level interface, easier syntax for common plots\n",
    "- **Method 2 (Graph Objects)**: More control over styling and customization\n",
    "- `barmode='group'` creates side-by-side bars for comparison\n",
    "- `color` parameter in px.bar automatically creates grouped bars\n",
    "- `template='plotly_white'` applies a clean white background theme\n",
    "- Both methods create interactive plots with hover information, zoom, and pan capabilities\n",
    "\n",
    "**Expected Output:** Two interactive bar charts showing product sales comparison between 2022 and 2023, with hover tooltips and interactive features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a45866",
   "metadata": {},
   "source": [
    "## 6. Create a DataFrame and add a new column based on an existing column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create initial DataFrame\n",
    "employee_data = {\n",
    "    'Name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Prince', 'Eve Wilson'],\n",
    "    'Department': ['Engineering', 'Marketing', 'Engineering', 'Sales', 'Marketing'],\n",
    "    'Salary': [75000, 55000, 82000, 62000, 58000],\n",
    "    'Years_Experience': [5, 3, 7, 4, 2],\n",
    "    'Performance_Score': [4.2, 3.8, 4.5, 4.0, 3.9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(employee_data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 1: Simple arithmetic operation\n",
    "df['Annual_Bonus'] = df['Salary'] * 0.1  # 10% bonus\n",
    "print(\"Added Annual_Bonus column (10% of salary):\")\n",
    "print(df[['Name', 'Salary', 'Annual_Bonus']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 2: Using conditional logic with np.where\n",
    "import numpy as np\n",
    "df['Salary_Category'] = np.where(df['Salary'] >= 70000, 'High', 'Standard')\n",
    "print(\"Added Salary_Category column using conditional logic:\")\n",
    "print(df[['Name', 'Salary', 'Salary_Category']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 3: Using pandas apply() with lambda function\n",
    "df['Experience_Level'] = df['Years_Experience'].apply(\n",
    "    lambda x: 'Senior' if x >= 6 else 'Mid' if x >= 3 else 'Junior'\n",
    ")\n",
    "print(\"Added Experience_Level column using apply():\")\n",
    "print(df[['Name', 'Years_Experience', 'Experience_Level']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 4: Complex calculation using multiple columns\n",
    "df['Total_Compensation'] = df['Salary'] + df['Annual_Bonus'] + (df['Performance_Score'] * 1000)\n",
    "print(\"Added Total_Compensation column (salary + bonus + performance bonus):\")\n",
    "print(df[['Name', 'Salary', 'Annual_Bonus', 'Performance_Score', 'Total_Compensation']])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Method 5: Using string operations\n",
    "df['First_Name'] = df['Name'].str.split().str[0]  # Extract first name\n",
    "print(\"Added First_Name column using string operations:\")\n",
    "print(df[['Name', 'First_Name']])\n",
    "\n",
    "# Display final DataFrame\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Final DataFrame with all new columns:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079bebd",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "This example demonstrates 5 different ways to create new columns:\n",
    "\n",
    "1. **Simple arithmetic**: `df['new_col'] = df['existing_col'] * 0.1`\n",
    "2. **Conditional logic**: `np.where()` for if-else conditions\n",
    "3. **Apply function**: `df['col'].apply(lambda x: ...)` for complex transformations\n",
    "4. **Multiple columns**: Combine multiple existing columns in calculations\n",
    "5. **String operations**: `str.split()` and `str[]` for text manipulation\n",
    "\n",
    "**Key Methods:**\n",
    "- Direct assignment with arithmetic operations\n",
    "- `np.where(condition, value_if_true, value_if_false)`\n",
    "- `apply()` with lambda functions for complex logic\n",
    "- String accessor methods with `.str`\n",
    "\n",
    "**Expected Output:** A DataFrame showing the progressive addition of new columns based on existing data, demonstrating various transformation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ffc29",
   "metadata": {},
   "source": [
    "## 7. Write a program to perform element-wise multiplication of two NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc74133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create two 1D arrays\n",
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "print(\"1D Arrays:\")\n",
    "print(f\"Array 1: {array1}\")\n",
    "print(f\"Array 2: {array2}\")\n",
    "\n",
    "# Element-wise multiplication for 1D arrays\n",
    "result_1d = array1 * array2\n",
    "print(f\"Element-wise multiplication: {result_1d}\")\n",
    "print()\n",
    "\n",
    "# Create two 2D arrays\n",
    "array_2d_1 = np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "array_2d_2 = np.array([[2, 2, 2],\n",
    "                       [3, 3, 3]])\n",
    "\n",
    "print(\"2D Arrays:\")\n",
    "print(\"Array 1:\")\n",
    "print(array_2d_1)\n",
    "print(\"Array 2:\")\n",
    "print(array_2d_2)\n",
    "\n",
    "# Element-wise multiplication for 2D arrays\n",
    "result_2d = array_2d_1 * array_2d_2\n",
    "print(\"Element-wise multiplication result:\")\n",
    "print(result_2d)\n",
    "print()\n",
    "\n",
    "# Alternative method using np.multiply()\n",
    "result_multiply = np.multiply(array_2d_1, array_2d_2)\n",
    "print(\"Using np.multiply() function:\")\n",
    "print(result_multiply)\n",
    "print()\n",
    "\n",
    "# Broadcasting example (different shapes)\n",
    "array_broadcast_1 = np.array([[1, 2, 3],\n",
    "                              [4, 5, 6]])\n",
    "array_broadcast_2 = np.array([10, 20, 30])  # 1D array\n",
    "\n",
    "print(\"Broadcasting example:\")\n",
    "print(\"Array 1 (2x3):\")\n",
    "print(array_broadcast_1)\n",
    "print(\"Array 2 (1x3):\")\n",
    "print(array_broadcast_2)\n",
    "\n",
    "result_broadcast = array_broadcast_1 * array_broadcast_2\n",
    "print(\"Element-wise multiplication with broadcasting:\")\n",
    "print(result_broadcast)\n",
    "print()\n",
    "\n",
    "# Verify element-wise vs matrix multiplication difference\n",
    "print(\"Comparison: Element-wise (*) vs Matrix multiplication (@):\")\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"Matrix B:\")\n",
    "print(B)\n",
    "print(\"Element-wise multiplication (A * B):\")\n",
    "print(A * B)\n",
    "print(\"Matrix multiplication (A @ B):\")\n",
    "print(A @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130bd06",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- **Element-wise multiplication**: `*` operator multiplies corresponding elements\n",
    "- **Alternative method**: `np.multiply()` function does the same operation\n",
    "- **Broadcasting**: NumPy automatically handles arrays of different shapes when possible\n",
    "- **Difference from matrix multiplication**: `*` is element-wise, `@` or `np.dot()` is matrix multiplication\n",
    "\n",
    "**Key Points:**\n",
    "- Arrays must have compatible shapes (same shape or broadcastable)\n",
    "- Element-wise: `[1,2] * [3,4] = [3,8]`\n",
    "- Matrix multiplication: `[[1,2]] @ [[3],[4]] = [[11]]`\n",
    "\n",
    "**Expected Output:** Demonstration of element-wise multiplication for 1D, 2D arrays, broadcasting, and comparison with matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3daf1",
   "metadata": {},
   "source": [
    "## 8. Create a line plot with multiple lines using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate x values\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# Generate multiple y series\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "y3 = np.sin(x) * np.exp(-x/5)  # Damped sine wave\n",
    "y4 = x * 0.1  # Linear trend\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot multiple lines with different styles\n",
    "plt.plot(x, y1, label='sin(x)', color='blue', linewidth=2, linestyle='-')\n",
    "plt.plot(x, y2, label='cos(x)', color='red', linewidth=2, linestyle='--')\n",
    "plt.plot(x, y3, label='damped sin(x)', color='green', linewidth=2, linestyle='-.')\n",
    "plt.plot(x, y4, label='linear trend', color='orange', linewidth=2, linestyle=':')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Multiple Line Plot Example', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('X values', fontsize=14)\n",
    "plt.ylabel('Y values', fontsize=14)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper right', fontsize=12, framealpha=0.9)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5)\n",
    "\n",
    "# Add annotations for interesting points\n",
    "plt.annotate('Maximum of cos(x)', \n",
    "             xy=(0, 1), xytext=(2, 1.3),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),\n",
    "             fontsize=10)\n",
    "\n",
    "# Customize tick parameters\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternative approach: subplots for comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left subplot: All lines together\n",
    "ax1.plot(x, y1, 'b-', label='sin(x)', linewidth=2)\n",
    "ax1.plot(x, y2, 'r--', label='cos(x)', linewidth=2)\n",
    "ax1.plot(x, y3, 'g-.', label='damped sin(x)', linewidth=2)\n",
    "ax1.set_title('Trigonometric Functions')\n",
    "ax1.set_xlabel('X values')\n",
    "ax1.set_ylabel('Y values')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right subplot: Linear vs exponential trends\n",
    "x_trend = np.linspace(0, 5, 50)\n",
    "linear = x_trend\n",
    "exponential = np.exp(x_trend/2)\n",
    "\n",
    "ax2.plot(x_trend, linear, 'b-', label='Linear (x)', linewidth=2)\n",
    "ax2.plot(x_trend, exponential, 'r-', label='Exponential (e^(x/2))', linewidth=2)\n",
    "ax2.set_title('Linear vs Exponential Growth')\n",
    "ax2.set_xlabel('X values')\n",
    "ax2.set_ylabel('Y values')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Line styles used:\")\n",
    "print(\"- Solid line: '-'\")\n",
    "print(\"- Dashed line: '--'\")\n",
    "print(\"- Dash-dot line: '-.'\")\n",
    "print(\"- Dotted line: ':'\")\n",
    "print(\"- Colors: 'blue', 'red', 'green', 'orange' or 'b', 'r', 'g', etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4aeea",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Multiple `plt.plot()` calls create multiple lines on the same axes\n",
    "- Different line styles: solid (-), dashed (--), dash-dot (-.), dotted (:)\n",
    "- `label` parameter creates entries for the legend\n",
    "- `plt.legend()` displays the legend with line labels\n",
    "- `plt.annotate()` adds text annotations with arrows\n",
    "- Subplots allow side-by-side comparison of different data\n",
    "\n",
    "**Expected Output:** Two visualizations - one showing multiple trigonometric functions on a single plot, and another showing subplots comparing different types of mathematical functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c628bd",
   "metadata": {},
   "source": [
    "## 9. Generate a Pandas DataFrame and filter rows where a column value is greater than a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample DataFrame\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Student_ID': range(1, 21),\n",
    "    'Name': [f'Student_{i}' for i in range(1, 21)],\n",
    "    'Math_Score': np.random.randint(60, 100, 20),\n",
    "    'Science_Score': np.random.randint(55, 95, 20),\n",
    "    'English_Score': np.random.randint(50, 100, 20),\n",
    "    'Age': np.random.randint(18, 25, 20),\n",
    "    'GPA': np.round(np.random.uniform(2.0, 4.0, 20), 2)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nTotal students: {len(df)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 1: Simple filtering - Math score > 80\n",
    "threshold_math = 80\n",
    "filtered_math = df[df['Math_Score'] > threshold_math]\n",
    "print(f\"Students with Math Score > {threshold_math}:\")\n",
    "print(filtered_math[['Name', 'Math_Score']])\n",
    "print(f\"Number of students: {len(filtered_math)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 2: Multiple conditions - GPA > 3.5 AND Age < 22\n",
    "high_gpa_young = df[(df['GPA'] > 3.5) & (df['Age'] < 22)]\n",
    "print(\"Students with GPA > 3.5 AND Age < 22:\")\n",
    "print(high_gpa_young[['Name', 'GPA', 'Age']])\n",
    "print(f\"Number of students: {len(high_gpa_young)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 3: Using query() method - more readable for complex conditions\n",
    "high_performers = df.query('Math_Score > 85 and Science_Score > 80 and English_Score > 75')\n",
    "print(\"High performers (Math>85, Science>80, English>75):\")\n",
    "print(high_performers[['Name', 'Math_Score', 'Science_Score', 'English_Score']])\n",
    "print(f\"Number of high performers: {len(high_performers)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 4: Using isin() for filtering with multiple values\n",
    "target_ages = [20, 21, 22]\n",
    "students_target_age = df[df['Age'].isin(target_ages)]\n",
    "print(f\"Students aged {target_ages}:\")\n",
    "print(students_target_age[['Name', 'Age']])\n",
    "print(f\"Number of students: {len(students_target_age)}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 5: Filtering with string operations\n",
    "# Let's filter students whose names contain specific patterns\n",
    "pattern_students = df[df['Name'].str.contains('1')]  # Names containing '1'\n",
    "print(\"Students with '1' in their name:\")\n",
    "print(pattern_students[['Name']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Summary statistics for filtered data\n",
    "print(\"Summary statistics for high performers:\")\n",
    "print(high_performers[['Math_Score', 'Science_Score', 'English_Score', 'GPA']].describe())\n",
    "\n",
    "# Advanced filtering: Top 25% by GPA\n",
    "gpa_75th_percentile = df['GPA'].quantile(0.75)\n",
    "top_25_percent = df[df['GPA'] >= gpa_75th_percentile]\n",
    "print(f\"\\nTop 25% students by GPA (GPA >= {gpa_75th_percentile:.2f}):\")\n",
    "print(top_25_percent[['Name', 'GPA']].sort_values('GPA', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5165d48",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Filtering Methods Demonstrated:**\n",
    "1. **Simple filtering**: `df[df['column'] > threshold]` - Basic comparison\n",
    "2. **Multiple conditions**: `df[(condition1) & (condition2)]` - Use `&` for AND, `|` for OR\n",
    "3. **Query method**: `df.query('condition')` - More readable for complex conditions\n",
    "4. **isin() method**: `df[df['column'].isin(values)]` - Check if values are in a list\n",
    "5. **String operations**: `df[df['column'].str.contains('pattern')]` - Text filtering\n",
    "6. **Quantile-based**: Using percentiles for dynamic thresholds\n",
    "\n",
    "**Key Points:**\n",
    "- Parentheses are required around conditions when using `&` and `|`\n",
    "- `query()` method uses string expressions and is often more readable\n",
    "- String methods are accessed via `.str` accessor\n",
    "- Filtering returns a new DataFrame with matching rows\n",
    "\n",
    "**Expected Output:** Various filtered subsets of student data based on different criteria, demonstrating the flexibility of Pandas filtering operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d14624",
   "metadata": {},
   "source": [
    "## 10. Create a histogram using Seaborn to visualize a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Generate sample data for visualization\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create different types of distributions\n",
    "normal_data = np.random.normal(100, 15, 1000)  # Normal distribution\n",
    "exponential_data = np.random.exponential(2, 1000)  # Exponential distribution\n",
    "bimodal_data = np.concatenate([np.random.normal(70, 10, 500), \n",
    "                               np.random.normal(130, 10, 500)])  # Bimodal\n",
    "\n",
    "# Create a DataFrame for grouped analysis\n",
    "data = {\n",
    "    'values': np.concatenate([normal_data[:300], normal_data[:300] + 20, normal_data[:300] + 40]),\n",
    "    'group': ['Group A'] * 300 + ['Group B'] * 300 + ['Group C'] * 300\n",
    "}\n",
    "df_grouped = pd.DataFrame(data)\n",
    "\n",
    "# Create subplots for different histogram examples\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Histogram Examples using Seaborn', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Basic histogram\n",
    "sns.histplot(normal_data, bins=30, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Basic Histogram\\n(Normal Distribution)')\n",
    "axes[0, 0].set_xlabel('Values')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Histogram with KDE (Kernel Density Estimation)\n",
    "sns.histplot(normal_data, bins=30, kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Histogram with KDE\\n(Density Curve)')\n",
    "axes[0, 1].set_xlabel('Values')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Multiple distributions comparison\n",
    "sns.histplot(normal_data, bins=30, alpha=0.5, label='Normal', ax=axes[0, 2])\n",
    "sns.histplot(exponential_data * 20 + 60, bins=30, alpha=0.5, label='Exponential', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Comparing Multiple Distributions')\n",
    "axes[0, 2].set_xlabel('Values')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Grouped histogram\n",
    "sns.histplot(data=df_grouped, x='values', hue='group', bins=25, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Grouped Histogram by Category')\n",
    "axes[1, 0].set_xlabel('Values')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 5. Normalized histogram (density)\n",
    "sns.histplot(bimodal_data, bins=40, stat='density', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Normalized Histogram\\n(Bimodal Distribution)')\n",
    "axes[1, 1].set_xlabel('Values')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "\n",
    "# 6. Step histogram\n",
    "sns.histplot(normal_data, bins=25, element='step', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Step Histogram')\n",
    "axes[1, 2].set_xlabel('Values')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional examples with different plot types\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Distribution plot (deprecated but still useful to know)\n",
    "sns.histplot(normal_data, kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution with KDE')\n",
    "\n",
    "# Box plot for comparison\n",
    "sns.boxplot(data=df_grouped, x='group', y='values', ax=axes[1])\n",
    "axes[1].set_title('Box Plot of Groups')\n",
    "\n",
    "# Violin plot combining histogram and box plot\n",
    "sns.violinplot(data=df_grouped, x='group', y='values', ax=axes[2])\n",
    "axes[2].set_title('Violin Plot of Groups')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"Statistical Summary of the Normal Distribution:\")\n",
    "print(f\"Mean: {np.mean(normal_data):.2f}\")\n",
    "print(f\"Standard Deviation: {np.std(normal_data):.2f}\")\n",
    "print(f\"Min: {np.min(normal_data):.2f}\")\n",
    "print(f\"Max: {np.max(normal_data):.2f}\")\n",
    "print(f\"Median: {np.median(normal_data):.2f}\")\n",
    "\n",
    "print(\"\\nGroup Statistics:\")\n",
    "print(df_grouped.groupby('group')['values'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8beb6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Histogram Types Demonstrated:**\n",
    "1. **Basic histogram**: `sns.histplot(data, bins=30)` - Simple frequency distribution\n",
    "2. **Histogram with KDE**: `kde=True` adds a smooth density curve\n",
    "3. **Multiple distributions**: Overlaying histograms with `alpha` for transparency\n",
    "4. **Grouped histogram**: `hue` parameter creates separate histograms by category\n",
    "5. **Normalized histogram**: `stat='density'` shows proportions instead of counts\n",
    "6. **Step histogram**: `element='step'` creates outline-only histogram\n",
    "\n",
    "**Key Parameters:**\n",
    "- `bins`: Number of bins (bars) in the histogram\n",
    "- `kde`: Add kernel density estimation curve\n",
    "- `stat`: Type of statistic ('count', 'density', 'probability')\n",
    "- `hue`: Group data by categorical variable\n",
    "- `alpha`: Transparency (0-1)\n",
    "- `element`: How to draw histogram ('bars', 'step', 'poly')\n",
    "\n",
    "**Expected Output:** Six different histogram visualizations showing various ways to display distribution data, plus box plots and violin plots for comparison. Statistical summaries provide numerical context for the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef01a4",
   "metadata": {},
   "source": [
    "## 11. Perform matrix multiplication using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f6a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Matrix Multiplication Examples using NumPy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Basic 2x2 matrix multiplication\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "print(\"Example 1: Basic 2x2 Matrix Multiplication\")\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "# Method 1: Using @ operator (recommended)\n",
    "result_at = A @ B\n",
    "print(\"\\nA @ B (using @ operator):\")\n",
    "print(result_at)\n",
    "\n",
    "# Method 2: Using np.dot() function\n",
    "result_dot = np.dot(A, B)\n",
    "print(\"\\nnp.dot(A, B):\")\n",
    "print(result_dot)\n",
    "\n",
    "# Method 3: Using .dot() method\n",
    "result_method = A.dot(B)\n",
    "print(\"\\nA.dot(B):\")\n",
    "print(result_method)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 2: Different sized matrices\n",
    "C = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])  # 2x3 matrix\n",
    "\n",
    "D = np.array([[7, 8],\n",
    "              [9, 10],\n",
    "              [11, 12]])  # 3x2 matrix\n",
    "\n",
    "print(\"Example 2: Different sized matrices (2x3) × (3x2)\")\n",
    "print(\"Matrix C (2x3):\")\n",
    "print(C)\n",
    "print(\"\\nMatrix D (3x2):\")\n",
    "print(D)\n",
    "\n",
    "result_cd = C @ D  # Results in 2x2 matrix\n",
    "print(\"\\nC @ D (result is 2x2):\")\n",
    "print(result_cd)\n",
    "\n",
    "# Show the calculation step by step for first element\n",
    "print(\"\\nStep-by-step calculation for result[0,0]:\")\n",
    "print(f\"C[0,:] = {C[0,:]}\")\n",
    "print(f\"D[:,0] = {D[:,0]}\")\n",
    "print(f\"Dot product: {C[0,:]} · {D[:,0]} = {np.dot(C[0,:], D[:,0])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 3: Matrix-vector multiplication\n",
    "E = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "v = np.array([1, 0, -1])\n",
    "\n",
    "print(\"Example 3: Matrix-Vector Multiplication\")\n",
    "print(\"Matrix E:\")\n",
    "print(E)\n",
    "print(f\"\\nVector v: {v}\")\n",
    "\n",
    "result_ev = E @ v\n",
    "print(f\"\\nE @ v = {result_ev}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 4: Multiple matrix multiplications\n",
    "F = np.array([[2, 0],\n",
    "              [1, 3]])\n",
    "\n",
    "G = np.array([[1, 4],\n",
    "              [2, 1]])\n",
    "\n",
    "H = np.array([[1, 0],\n",
    "              [0, 2]])\n",
    "\n",
    "print(\"Example 4: Chain Matrix Multiplication\")\n",
    "print(\"Matrix F:\")\n",
    "print(F)\n",
    "print(\"\\nMatrix G:\")\n",
    "print(G)\n",
    "print(\"\\nMatrix H:\")\n",
    "print(H)\n",
    "\n",
    "# Chain multiplication: (F @ G) @ H\n",
    "result_chain = F @ G @ H\n",
    "print(\"\\nF @ G @ H:\")\n",
    "print(result_chain)\n",
    "\n",
    "# Show intermediate step\n",
    "intermediate = F @ G\n",
    "print(f\"\\nIntermediate result (F @ G):\")\n",
    "print(intermediate)\n",
    "print(f\"\\nFinal result ((F @ G) @ H):\")\n",
    "print(intermediate @ H)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 5: Identity matrix and inverse\n",
    "I = np.eye(3)  # 3x3 identity matrix\n",
    "M = np.array([[2, 1, 0],\n",
    "              [1, 2, 1],\n",
    "              [0, 1, 2]])\n",
    "\n",
    "print(\"Example 5: Identity Matrix and Matrix Properties\")\n",
    "print(\"Identity matrix I:\")\n",
    "print(I)\n",
    "print(\"\\nMatrix M:\")\n",
    "print(M)\n",
    "\n",
    "# Multiplication with identity\n",
    "result_identity = M @ I\n",
    "print(\"\\nM @ I (should equal M):\")\n",
    "print(result_identity)\n",
    "\n",
    "# Matrix inverse (if it exists)\n",
    "try:\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    print(\"\\nInverse of M:\")\n",
    "    print(M_inv)\n",
    "    \n",
    "    # Verify: M @ M^(-1) should equal identity\n",
    "    verification = M @ M_inv\n",
    "    print(\"\\nM @ M^(-1) (should be close to identity):\")\n",
    "    print(np.round(verification, 10))  # Round to avoid floating point errors\n",
    "    \n",
    "except np.linalg.LinAlgError:\n",
    "    print(\"\\nMatrix M is not invertible (singular matrix)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Example 6: Batch matrix multiplication\n",
    "print(\"Example 6: Batch Operations\")\n",
    "# Create arrays of matrices\n",
    "batch_A = np.random.randint(1, 5, (3, 2, 2))  # 3 matrices of size 2x2\n",
    "batch_B = np.random.randint(1, 5, (3, 2, 2))  # 3 matrices of size 2x2\n",
    "\n",
    "print(\"Batch of 3 matrices A:\")\n",
    "for i, matrix in enumerate(batch_A):\n",
    "    print(f\"A[{i}]:\")\n",
    "    print(matrix)\n",
    "\n",
    "print(\"\\nBatch matrix multiplication using np.matmul:\")\n",
    "batch_result = np.matmul(batch_A, batch_B)  # Multiply corresponding matrices\n",
    "print(\"Results:\")\n",
    "for i, result in enumerate(batch_result):\n",
    "    print(f\"A[{i}] @ B[{i}]:\")\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdbc55",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Matrix Multiplication Methods:**\n",
    "1. **@ operator**: `A @ B` - Recommended modern approach (Python 3.5+)\n",
    "2. **np.dot()**: `np.dot(A, B)` - Traditional function approach\n",
    "3. **dot() method**: `A.dot(B)` - Object method approach\n",
    "4. **np.matmul()**: `np.matmul(A, B)` - Explicit matrix multiplication function\n",
    "\n",
    "**Key Rules:**\n",
    "- Matrix dimensions must be compatible: (m×n) × (n×p) → (m×p)\n",
    "- Order matters: A @ B ≠ B @ A (generally)\n",
    "- Identity matrix: A @ I = I @ A = A\n",
    "- Inverse: A @ A⁻¹ = A⁻¹ @ A = I (if A is invertible)\n",
    "\n",
    "**Applications Shown:**\n",
    "- Basic 2×2 multiplication\n",
    "- Different sized matrices\n",
    "- Matrix-vector multiplication\n",
    "- Chain multiplication\n",
    "- Identity and inverse operations\n",
    "- Batch processing of multiple matrices\n",
    "\n",
    "**Expected Output:** Comprehensive examples showing various matrix multiplication scenarios with step-by-step calculations and verification of mathematical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362127b",
   "metadata": {},
   "source": [
    "## 12. Use Pandas to load a CSV file and display its first 5 rows. (Create sample CSV first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cf9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Step 1: Create a sample CSV file first\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample employee data\n",
    "sample_data = {\n",
    "    'Employee_ID': range(1001, 1051),\n",
    "    'Name': [f'Employee_{i:02d}' for i in range(1, 51)],\n",
    "    'Department': np.random.choice(['HR', 'Engineering', 'Marketing', 'Sales', 'Finance'], 50),\n",
    "    'Salary': np.random.randint(40000, 120000, 50),\n",
    "    'Years_Experience': np.random.randint(0, 20, 50),\n",
    "    'Performance_Rating': np.round(np.random.uniform(2.0, 5.0, 50), 1),\n",
    "    'City': np.random.choice(['New York', 'San Francisco', 'Chicago', 'Boston', 'Seattle'], 50),\n",
    "    'Hire_Date': pd.date_range('2010-01-01', '2023-12-31', periods=50).strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "csv_filename = 'sample_employee_data.csv'\n",
    "df_sample.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"✓ Created sample CSV file: {csv_filename}\")\n",
    "print(f\"File size: {os.path.getsize(csv_filename)} bytes\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 2: Load the CSV file using Pandas\n",
    "print(\"Loading CSV file using pd.read_csv()...\")\n",
    "\n",
    "# Method 1: Basic loading\n",
    "df = pd.read_csv(csv_filename)\n",
    "print(f\"Successfully loaded {csv_filename}\")\n",
    "print(f\"DataFrame shape: {df.shape} (rows, columns)\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Step 3: Display first 5 rows\n",
    "print(\"First 5 rows using head():\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Additional useful methods when working with CSV files\n",
    "print(\"Basic information about the dataset:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"Data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show different ways to explore the data\n",
    "print(\"First 3 rows:\")\n",
    "print(df.head(3))\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(df.tail())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Method 2: Loading with specific parameters\n",
    "print(\"Advanced CSV loading options:\")\n",
    "\n",
    "# Load with custom parameters\n",
    "df_custom = pd.read_csv(csv_filename, \n",
    "                       parse_dates=['Hire_Date'],  # Parse date column\n",
    "                       dtype={'Employee_ID': str})  # Specify data type\n",
    "\n",
    "print(\"After custom loading with date parsing:\")\n",
    "print(df_custom.dtypes)\n",
    "print(\"\\nFirst 5 rows with parsed dates:\")\n",
    "print(df_custom.head())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"Summary statistics of numerical columns:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Show missing values check\n",
    "print(\"Missing values check:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"Total missing values: {missing_values.sum()}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Sample queries on the loaded data\n",
    "print(\"Sample data analysis:\")\n",
    "print(\"1. Average salary by department:\")\n",
    "avg_salary = df.groupby('Department')['Salary'].mean().sort_values(ascending=False)\n",
    "print(avg_salary)\n",
    "\n",
    "print(\"\\n2. Top 5 highest paid employees:\")\n",
    "top_earners = df.nlargest(5, 'Salary')[['Name', 'Department', 'Salary']]\n",
    "print(top_earners)\n",
    "\n",
    "print(\"\\n3. Employee count by city:\")\n",
    "city_counts = df['City'].value_counts()\n",
    "print(city_counts)\n",
    "\n",
    "# Cleanup: Remove the created CSV file (optional)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Note: Sample CSV file '{csv_filename}' has been created in the current directory.\")\n",
    "print(\"You can examine it or delete it after running this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a162ca",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Process Demonstrated:**\n",
    "1. **Create sample CSV**: Generate realistic employee data and save to CSV file\n",
    "2. **Load CSV**: Use `pd.read_csv()` to load the file into a DataFrame\n",
    "3. **Display data**: Use `head()` to show first 5 rows\n",
    "4. **Explore data**: Various methods to understand the dataset\n",
    "\n",
    "**Key Methods for CSV Loading:**\n",
    "- `pd.read_csv(filename)`: Basic CSV loading\n",
    "- `parse_dates`: Automatically parse date columns\n",
    "- `dtype`: Specify data types for columns\n",
    "- `head(n)`: Display first n rows (default 5)\n",
    "- `tail(n)`: Display last n rows\n",
    "- `info()`: Overview of DataFrame structure\n",
    "- `describe()`: Statistical summary\n",
    "\n",
    "**Additional Analysis Methods:**\n",
    "- `shape`: Get dimensions (rows, columns)\n",
    "- `columns`: Get column names\n",
    "- `dtypes`: Get data types\n",
    "- `isnull().sum()`: Check for missing values\n",
    "- `groupby()`: Group data for analysis\n",
    "\n",
    "**Expected Output:** Complete workflow from creating a CSV file to loading and analyzing it, including data exploration and summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0162b9d",
   "metadata": {},
   "source": [
    "## 13. Create a 3D scatter plot using Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Creating 3D Scatter Plots using Plotly\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Basic 3D scatter plot with random data\n",
    "n_points = 100\n",
    "\n",
    "# Generate 3D data with some patterns\n",
    "x = np.random.normal(0, 1, n_points)\n",
    "y = np.random.normal(0, 1, n_points)\n",
    "z = x**2 + y**2 + np.random.normal(0, 0.1, n_points)  # Paraboloid with noise\n",
    "\n",
    "# Create basic 3D scatter plot\n",
    "fig1 = px.scatter_3d(x=x, y=y, z=z,\n",
    "                     title='Basic 3D Scatter Plot',\n",
    "                     labels={'x': 'X Axis', 'y': 'Y Axis', 'z': 'Z Axis'})\n",
    "\n",
    "fig1.update_traces(marker=dict(size=5, opacity=0.8))\n",
    "fig1.show()\n",
    "\n",
    "print(\"Example 1: Basic 3D scatter plot created\")\n",
    "print(\"- Interactive plot with zoom, rotate, and pan capabilities\")\n",
    "print(\"- Data shows a paraboloid pattern (z = x² + y² + noise)\")\n",
    "print()\n",
    "\n",
    "# Example 2: 3D scatter plot with color coding and categories\n",
    "categories = np.random.choice(['Group A', 'Group B', 'Group C'], n_points)\n",
    "colors = np.random.uniform(0, 100, n_points)  # Color scale values\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "df_3d = pd.DataFrame({\n",
    "    'X': x,\n",
    "    'Y': y, \n",
    "    'Z': z,\n",
    "    'Category': categories,\n",
    "    'Color_Value': colors,\n",
    "    'Size': np.random.uniform(5, 15, n_points)\n",
    "})\n",
    "\n",
    "fig2 = px.scatter_3d(df_3d, x='X', y='Y', z='Z',\n",
    "                     color='Category',  # Color by category\n",
    "                     size='Size',       # Size by value\n",
    "                     hover_data=['Color_Value'],  # Additional hover info\n",
    "                     title='3D Scatter Plot with Categories and Sizing',\n",
    "                     color_discrete_sequence=['red', 'blue', 'green'])\n",
    "\n",
    "fig2.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='X Coordinate',\n",
    "        yaxis_title='Y Coordinate', \n",
    "        zaxis_title='Z Coordinate',\n",
    "        bgcolor='white'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(\"Example 2: Enhanced 3D scatter plot with:\")\n",
    "print(\"- Color coding by category\")\n",
    "print(\"- Variable point sizes\")\n",
    "print(\"- Custom hover information\")\n",
    "print(\"- Styled axes and background\")\n",
    "print()\n",
    "\n",
    "# Example 3: Mathematical surface visualization\n",
    "# Create a more complex 3D dataset\n",
    "theta = np.linspace(0, 2*np.pi, 50)\n",
    "phi = np.linspace(0, np.pi, 50)\n",
    "theta_grid, phi_grid = np.meshgrid(theta, phi)\n",
    "\n",
    "# Convert spherical coordinates to Cartesian (sphere surface)\n",
    "x_sphere = np.sin(phi_grid) * np.cos(theta_grid)\n",
    "y_sphere = np.sin(phi_grid) * np.sin(theta_grid) \n",
    "z_sphere = np.cos(phi_grid)\n",
    "\n",
    "# Flatten arrays for scatter plot\n",
    "x_flat = x_sphere.flatten()\n",
    "y_flat = y_sphere.flatten()\n",
    "z_flat = z_sphere.flatten()\n",
    "\n",
    "# Add color based on height (z-coordinate)\n",
    "colors_sphere = z_flat\n",
    "\n",
    "fig3 = go.Figure(data=[go.Scatter3d(\n",
    "    x=x_flat,\n",
    "    y=y_flat,\n",
    "    z=z_flat,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=3,\n",
    "        color=colors_sphere,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Height (Z)\")\n",
    "    ),\n",
    "    text=[f'Point ({x:.2f}, {y:.2f}, {z:.2f})' for x, y, z in zip(x_flat, y_flat, z_flat)],\n",
    "    hovertemplate='<b>Coordinates</b><br>' +\n",
    "                  'X: %{x:.3f}<br>' +\n",
    "                  'Y: %{y:.3f}<br>' +\n",
    "                  'Z: %{z:.3f}<br>' +\n",
    "                  '<extra></extra>'\n",
    ")])\n",
    "\n",
    "fig3.update_layout(\n",
    "    title='3D Sphere Surface with Color Gradient',\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z',\n",
    "        aspectmode='cube',  # Equal aspect ratio\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0),\n",
    "            eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "        )\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig3.show()\n",
    "\n",
    "print(\"Example 3: Mathematical sphere surface\")\n",
    "print(\"- Points arranged on a sphere surface\")\n",
    "print(\"- Color gradient based on height (z-coordinate)\")\n",
    "print(\"- Custom hover templates\")\n",
    "print(\"- Equal aspect ratio for true sphere appearance\")\n",
    "print()\n",
    "\n",
    "# Example 4: Real-world data simulation (3D clustering)\n",
    "print(\"Example 4: Simulated real-world data (customer segmentation)\")\n",
    "\n",
    "# Simulate customer data with 3 clusters\n",
    "np.random.seed(123)\n",
    "\n",
    "# Cluster centers\n",
    "centers = [(2, 2, 2), (6, 6, 2), (2, 6, 6)]\n",
    "cluster_data = []\n",
    "\n",
    "for i, (cx, cy, cz) in enumerate(centers):\n",
    "    # Generate points around each center\n",
    "    n_cluster = 50\n",
    "    cluster_x = np.random.normal(cx, 1, n_cluster)\n",
    "    cluster_y = np.random.normal(cy, 1, n_cluster) \n",
    "    cluster_z = np.random.normal(cz, 1, n_cluster)\n",
    "    \n",
    "    cluster_df = pd.DataFrame({\n",
    "        'Income': cluster_x * 10000,  # Scale to realistic income\n",
    "        'Spending': cluster_y * 1000,  # Scale to spending\n",
    "        'Age': cluster_z * 8 + 20,     # Scale to age range\n",
    "        'Cluster': f'Segment {i+1}',\n",
    "        'Customer_ID': range(i*n_cluster, (i+1)*n_cluster)\n",
    "    })\n",
    "    cluster_data.append(cluster_df)\n",
    "\n",
    "# Combine all clusters\n",
    "df_customers = pd.concat(cluster_data, ignore_index=True)\n",
    "\n",
    "fig4 = px.scatter_3d(df_customers, \n",
    "                     x='Income', y='Spending', z='Age',\n",
    "                     color='Cluster',\n",
    "                     title='Customer Segmentation Analysis (3D)',\n",
    "                     labels={\n",
    "                         'Income': 'Annual Income ($)',\n",
    "                         'Spending': 'Annual Spending ($)', \n",
    "                         'Age': 'Age (years)'\n",
    "                     },\n",
    "                     hover_data=['Customer_ID'])\n",
    "\n",
    "fig4.update_traces(marker=dict(size=6, opacity=0.8))\n",
    "fig4.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='Annual Income ($)',\n",
    "        yaxis_title='Annual Spending ($)',\n",
    "        zaxis_title='Age (years)'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig4.show()\n",
    "\n",
    "print(\"Customer segmentation results:\")\n",
    "print(f\"Total customers: {len(df_customers)}\")\n",
    "print(\"Segments identified:\", df_customers['Cluster'].unique())\n",
    "print(\"\\nSegment characteristics:\")\n",
    "print(df_customers.groupby('Cluster')[['Income', 'Spending', 'Age']].mean().round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3D Scatter Plot Features Demonstrated:\")\n",
    "print(\"1. Basic 3D scatter plots with Plotly Express\")\n",
    "print(\"2. Color coding and sizing by variables\")\n",
    "print(\"3. Mathematical surface visualization\")\n",
    "print(\"4. Real-world clustering example\")\n",
    "print(\"5. Interactive features: zoom, rotate, pan, hover\")\n",
    "print(\"6. Custom styling and layouts\")\n",
    "print(\"7. Multiple data encoding methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcdafce",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**3D Scatter Plot Methods:**\n",
    "1. **Plotly Express**: `px.scatter_3d()` - High-level, easy-to-use interface\n",
    "2. **Graph Objects**: `go.Scatter3d()` - More control and customization options\n",
    "\n",
    "**Key Features Demonstrated:**\n",
    "- **Basic 3D plotting**: x, y, z coordinates with interactive controls\n",
    "- **Color coding**: `color` parameter for categorical or continuous variables\n",
    "- **Size variation**: `size` parameter for additional data dimension\n",
    "- **Hover information**: Custom tooltips with `hover_data` and `hovertemplate`\n",
    "- **Mathematical surfaces**: Sphere generation using spherical coordinates\n",
    "- **Real-world applications**: Customer segmentation clustering\n",
    "\n",
    "**Interactive Features:**\n",
    "- **Rotation**: Click and drag to rotate the 3D view\n",
    "- **Zoom**: Scroll to zoom in/out\n",
    "- **Pan**: Shift+drag to pan the view\n",
    "- **Hover**: Detailed information on mouse hover\n",
    "- **Camera control**: Programmatic view angle setting\n",
    "\n",
    "**Styling Options:**\n",
    "- Color scales (`colorscale='Viridis'`)\n",
    "- Marker properties (size, opacity)\n",
    "- Axis labels and titles\n",
    "- Background colors and layout\n",
    "- Equal aspect ratio (`aspectmode='cube'`)\n",
    "\n",
    "**Expected Output:** Four different 3D scatter plots showcasing various visualization techniques, from basic plots to complex real-world data analysis with customer segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4ab62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dd9f35e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
